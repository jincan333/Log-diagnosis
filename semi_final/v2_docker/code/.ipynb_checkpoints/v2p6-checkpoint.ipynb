{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ac94bf",
   "metadata": {},
   "source": [
    "# 对无日志标签进行数据分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d587f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "from logging import handlers\n",
    "from datetime import datetime, date\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import traceback\n",
    "import re\n",
    "import random\n",
    "\n",
    "\n",
    "## 日志格式设置\n",
    "# 日志级别关系映射\n",
    "level_relations = {\n",
    "    'debug': logging.DEBUG,\n",
    "    'info': logging.INFO,\n",
    "    'warning': logging.WARNING,\n",
    "    'error': logging.ERROR,\n",
    "    'crit': logging.CRITICAL\n",
    "}\n",
    "def get_logger(filename, level='info'):\n",
    "    # 创建日志对象\n",
    "    log = logging.getLogger(filename)\n",
    "    # 设置日志级别\n",
    "    log.setLevel(level_relations.get(level))\n",
    "    # 日志输出格式\n",
    "    fmt = logging.Formatter('%(asctime)s %(thread)d %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s')\n",
    "    # 输出到控制台\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(fmt)\n",
    "    # 输出到文件\n",
    "    # 日志文件按天进行保存，每天一个日志文件\n",
    "    file_handler = handlers.TimedRotatingFileHandler(filename=filename, when='D', backupCount=1, encoding='utf-8')\n",
    "    # 按照大小自动分割日志文件，一旦达到指定的大小重新生成文件\n",
    "    # file_handler = handlers.RotatingFileHandler(filename=filename, maxBytes=1*1024*1024*1024, backupCount=1, encoding='utf-8')\n",
    "    file_handler.setFormatter(fmt)\n",
    "    if not log.handlers:\n",
    "        log.addHandler(console_handler)\n",
    "        log.addHandler(file_handler)\n",
    "    return log\n",
    "\n",
    "# sn分组后，本次报错和上次报错之间的日志匹配到本次报错\n",
    "def divideLogByFaultTime(log_label_df: pd.DataFrame):\n",
    "    log_correspond_label_df = pd.DataFrame(columns=['sn', 'fault_time', 'msg', 'time', 'server_model', 'label'])\n",
    "    no_label_log_list = []\n",
    "    log_label_df = log_label_df.reset_index(drop=True)\n",
    "\n",
    "    for sn, log in log_label_df.groupby('sn'):\n",
    "        if len(log[log['label'] != '']) == 0:\n",
    "            no_label_log_list.append(log)\n",
    "        elif len(log[log['label'] != '']) == 1:\n",
    "            msg_df = log[log['label'] == '']\n",
    "            msg_df['label'] = log[log['label'] != '']['label'].iloc[0]\n",
    "            msg_df['fault_time'] = log[log['label'] != '']['time'].iloc[0]\n",
    "            log_correspond_label_df = pd.concat([log_correspond_label_df, msg_df])\n",
    "        else:\n",
    "            # 使用index的顺序取数时，要注意index必须按所需的顺序排列\n",
    "            cutoff_index = [-1] + log.loc[log['label'] != ''].index.tolist() + [log.index.tolist()[-1] + 1]\n",
    "            for kth in range(len(cutoff_index) - 1):\n",
    "                temp_log = log.loc[(log.index <= cutoff_index[kth + 1]) & (log.index > cutoff_index[kth])]\n",
    "                if len(temp_log) > 0:\n",
    "                    if len(temp_log[temp_log['label'] != '']) == 0:\n",
    "                        no_label_log_list.append(temp_log)\n",
    "                    # 只有标签，没有日志的数据，把标签的部分数据直接作为日志\n",
    "                    elif len(temp_log) == 1:\n",
    "                        msg_df = temp_log\n",
    "                        msg_df['fault_time'] = temp_log[temp_log['label'] != '']['time'].iloc[0]\n",
    "                        log_correspond_label_df = pd.concat([log_correspond_label_df, msg_df])\n",
    "                    else:\n",
    "                        msg_df = temp_log[temp_log['label'] == '']\n",
    "                        msg_df['label'] = temp_log[temp_log['label'] != '']['label'].iloc[0]\n",
    "                        msg_df['fault_time'] = temp_log[temp_log['label'] != '']['time'].iloc[0]\n",
    "                        log_correspond_label_df = pd.concat([log_correspond_label_df, msg_df])\n",
    "    return log_correspond_label_df, no_label_log_list\n",
    "\n",
    "# sn分组后，按照最近邻+时间间隔划分日志数据\n",
    "def divideLogByNearestTime(log_label_df: pd.DataFrame):\n",
    "    log_correspond_label_df = pd.DataFrame(columns=['sn', 'fault_time', 'msg', 'time', 'server_model', 'label'])\n",
    "    origin_label_df = log_label_df[log_label_df['fault_time'] != '']\n",
    "    no_label_log_list = []\n",
    "    cutoff = 10 * 3600\n",
    "\n",
    "    for sn, log in log_label_df.groupby('sn'):\n",
    "        if len(log[log['label'] != '']) == 0:\n",
    "            no_label_log_list.append(log)\n",
    "        elif len(log[log['label'] != '']) == 1:\n",
    "            msg_df = log[log['label'] == '']\n",
    "            if len(msg_df) > 0:\n",
    "                msg_df['label'] = log[log['label'] != '']['label'].iloc[0]\n",
    "                msg_df['fault_time'] = log[log['label'] != '']['time'].iloc[0]\n",
    "                log_correspond_label_df = pd.concat([log_correspond_label_df, msg_df])\n",
    "        else:\n",
    "            lable_df = log[log['label'] != '']\n",
    "            msg_df = log[log['label'] == '']\n",
    "            for msg_item in msg_df.iterrows():\n",
    "                previous_delta_time = 1000 * 24 * 3600\n",
    "                for lable_item in lable_df.iterrows():\n",
    "                    now_delta_time = abs(datetime.strptime(lable_item[1]['time'],'%Y-%m-%d %H:%M:%S'\n",
    "                        ) - datetime.strptime(msg_item[1]['time'],'%Y-%m-%d %H:%M:%S'))\n",
    "                    if now_delta_time.days * 24 * 3600 + now_delta_time.seconds < previous_delta_time:\n",
    "                        previous_delta_time = now_delta_time.days * 24 * 3600 + now_delta_time.seconds\n",
    "                        final_lable = lable_item[1]\n",
    "                        if previous_delta_time < cutoff:\n",
    "                            msg_item[1]['fault_time'] = lable_item[1]['time']\n",
    "                            msg_item[1]['label'] = lable_item[1]['label']\n",
    "            log_correspond_label_df = pd.concat([log_correspond_label_df, msg_df]) \n",
    "    log_correspond_label_df = log_correspond_label_df[log_correspond_label_df['label'] != '']\n",
    "    # 找出没有匹配到日志的标签并将其添加到 日志标签映射表 中\n",
    "    temp_df = pd.concat([log_correspond_label_df, origin_label_df])\n",
    "    sn_list = []\n",
    "    fault_time_list = []\n",
    "    msg_list = []\n",
    "    time_list = []\n",
    "    server_model_list = []\n",
    "    label_list = []\n",
    "    for g in temp_df.groupby(['sn', 'fault_time', 'label']):\n",
    "        if len(g[1]) == 1:\n",
    "            sn_list.append(g[0][0])\n",
    "            fault_time_list.append(g[0][1])\n",
    "            msg_list.append('')\n",
    "            time_list.append(g[1]['time'].iloc[0])\n",
    "            server_model_list.append(g[1]['server_model'].iloc[0])\n",
    "            label_list.append(g[0][2])\n",
    "    no_log_label_df  = pd.DataFrame({\n",
    "        'sn': sn_list,\n",
    "        'fault_time': fault_time_list,\n",
    "        'msg': msg_list,\n",
    "        'time': time_list,\n",
    "        'server_model': server_model_list,\n",
    "        'label': label_list\n",
    "    })\n",
    "    log_correspond_label_df = pd.concat([log_correspond_label_df, no_log_label_df])\n",
    "    return log_correspond_label_df, no_label_log_list\n",
    "\n",
    "# 计算统计特征\n",
    "def calculateStatisticFeature(log_correspond_label_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    use_log_label_df = log_correspond_label_df\n",
    "\n",
    "    use_log_label_df['msg_hour'] = use_log_label_df['time'].apply(\n",
    "        lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").hour)\n",
    "    use_log_label_df['msg_minute'] = use_log_label_df['time'].apply(\n",
    "        lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").minute)\n",
    "    use_log_label_df['fault_hour'] = use_log_label_df['fault_time'].apply(\n",
    "        lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").hour)\n",
    "    use_log_label_df['fault_minute'] = use_log_label_df['fault_time'].apply(\n",
    "        lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").minute)\n",
    "\n",
    "    # 0408新增\n",
    "    # 最近一次日志时间距报错时间间隔，单位秒\n",
    "    nearest_msg_fault_time_delta_list = []\n",
    "    # 日志不去重时长度1,2,3,4日志数量统计\n",
    "    all_msg_1_cnt_list = []\n",
    "    all_msg_2_cnt_list = []\n",
    "    all_msg_3_cnt_list = []\n",
    "    all_msg_4_cnt_list = []\n",
    "\n",
    "    fault_minute_list = []\n",
    "    msg_1_cnt_list = []\n",
    "    msg_2_cnt_list = []\n",
    "    msg_3_cnt_list = []\n",
    "    msg_4_cnt_list = []\n",
    "    msg_hour_max_list = []\n",
    "    msg_hour_min_list = []\n",
    "    msg_hour_avg_list = []\n",
    "    msg_hour_median_list = []\n",
    "    msg_hour_mode_list = []\n",
    "    msg_minute_max_list = []\n",
    "    msg_minute_min_list = []\n",
    "    msg_minute_avg_list = []\n",
    "    msg_minute_median_list = []\n",
    "    msg_minute_mode_list = []\n",
    "\n",
    "    sn_list = []\n",
    "    server_model_list = []\n",
    "    msg_log_list = []\n",
    "    msg_cnt_list = []\n",
    "    fault_hour_list = []\n",
    "    label_list = []\n",
    "    fault_time_list = []\n",
    "    for msg_log_df in use_log_label_df.groupby(['sn', 'fault_time', 'label']):\n",
    "        msg_log_str = ''\n",
    "        all_msg_1_cnt = 0\n",
    "        all_msg_2_cnt = 0\n",
    "        all_msg_3_cnt = 0\n",
    "        all_msg_4_cnt = 0\n",
    "        msg_1_cnt = 0\n",
    "        msg_2_cnt = 0\n",
    "        msg_3_cnt = 0\n",
    "        msg_4_cnt = 0\n",
    "        for info in msg_log_df[1]['msg']:\n",
    "            if info == info:\n",
    "                if len(info.split('|')) == 1:\n",
    "                    all_msg_1_cnt += 1\n",
    "                elif len(info.split('|')) == 2:\n",
    "                    all_msg_2_cnt += 1\n",
    "                elif len(info.split('|')) == 3:\n",
    "                    all_msg_3_cnt += 1\n",
    "                else:\n",
    "                    all_msg_4_cnt += 1\n",
    "        for info in msg_log_df[1]['msg'].drop_duplicates():\n",
    "            if info == info:\n",
    "                msg_log_str = msg_log_str + info.lower() + '.'\n",
    "                if len(info.split('|')) == 1:\n",
    "                    msg_1_cnt += 1\n",
    "                elif len(info.split('|')) == 2:\n",
    "                    msg_2_cnt += 1\n",
    "                elif len(info.split('|')) == 3:\n",
    "                    msg_3_cnt += 1\n",
    "                else:\n",
    "                    msg_4_cnt += 1\n",
    "        nearest_msg_fault_time_delta = abs(datetime.strptime(msg_log_df[1].iloc[-1]['time'], '%Y-%m-%d %H:%M:%S'\n",
    "                                                             ) - datetime.strptime(msg_log_df[0][1],\n",
    "                                                                                   '%Y-%m-%d %H:%M:%S'))\n",
    "        nearest_msg_fault_time_delta = nearest_msg_fault_time_delta.days * 24 * 3600 + nearest_msg_fault_time_delta.seconds\n",
    "        sm = int(msg_log_df[1].iloc[0]['server_model'][2:])\n",
    "\n",
    "        sn_list.append(msg_log_df[0][0])\n",
    "        fault_time_list.append(msg_log_df[0][1])\n",
    "        label_list.append(msg_log_df[0][2])\n",
    "\n",
    "        nearest_msg_fault_time_delta_list.append(nearest_msg_fault_time_delta)\n",
    "        server_model_list.append(sm)\n",
    "        msg_log_list.append(msg_log_str)\n",
    "        msg_cnt_list.append(len(msg_log_df[1]))\n",
    "\n",
    "        fault_hour_list.append(msg_log_df[1].iloc[0]['fault_hour'])\n",
    "        fault_minute_list.append(msg_log_df[1].iloc[0]['fault_minute'])\n",
    "\n",
    "        all_msg_1_cnt_list.append(all_msg_1_cnt)\n",
    "        all_msg_2_cnt_list.append(all_msg_2_cnt)\n",
    "        all_msg_3_cnt_list.append(all_msg_3_cnt)\n",
    "        all_msg_4_cnt_list.append(all_msg_4_cnt)\n",
    "\n",
    "        msg_1_cnt_list.append(msg_1_cnt)\n",
    "        msg_2_cnt_list.append(msg_2_cnt)\n",
    "        msg_3_cnt_list.append(msg_3_cnt)\n",
    "        msg_4_cnt_list.append(msg_4_cnt)\n",
    "\n",
    "        msg_hour_max_list.append(msg_log_df[1]['msg_hour'].max())\n",
    "        msg_hour_min_list.append(msg_log_df[1]['msg_hour'].min())\n",
    "        msg_hour_avg_list.append(msg_log_df[1]['msg_hour'].mean())\n",
    "        msg_hour_median_list.append(msg_log_df[1]['msg_hour'].median())\n",
    "        msg_hour_mode_list.append(msg_log_df[1]['msg_hour'].mode()[0])\n",
    "\n",
    "        msg_minute_max_list.append(msg_log_df[1]['msg_minute'].max())\n",
    "        msg_minute_min_list.append(msg_log_df[1]['msg_minute'].min())\n",
    "        msg_minute_avg_list.append(msg_log_df[1]['msg_minute'].mean())\n",
    "        msg_minute_median_list.append(msg_log_df[1]['msg_minute'].median())\n",
    "        msg_minute_mode_list.append(msg_log_df[1]['msg_minute'].mode()[0])\n",
    "\n",
    "    msg_log_label_df = pd.DataFrame(\n",
    "        {\n",
    "            'sn': sn_list,\n",
    "            'fault_time': fault_time_list,\n",
    "            'server_model': server_model_list,\n",
    "            'msg_cnt': msg_cnt_list,\n",
    "            'fault_hour': fault_hour_list,\n",
    "            'fault_minute': fault_minute_list,\n",
    "            'nearest_msg_fault_time_delta': nearest_msg_fault_time_delta_list,\n",
    "            'all_msg_1_cnt': all_msg_1_cnt_list,\n",
    "            'all_msg_2_cnt': all_msg_2_cnt_list,\n",
    "            'all_msg_3_cnt': all_msg_3_cnt_list,\n",
    "            'all_msg_4_cnt': all_msg_4_cnt_list,\n",
    "            'msg_1_cnt': msg_1_cnt_list,\n",
    "            'msg_2_cnt': msg_2_cnt_list,\n",
    "            'msg_3_cnt': msg_3_cnt_list,\n",
    "            'msg_4_cnt': msg_4_cnt_list,\n",
    "            'msg_hour_max': msg_hour_max_list,\n",
    "            'msg_hour_min': msg_hour_min_list,\n",
    "            'msg_hour_avg': msg_hour_avg_list,\n",
    "            'msg_hour_median': msg_hour_median_list,\n",
    "            'msg_hour_mode': msg_hour_mode_list,\n",
    "            'msg_minute_max': msg_minute_max_list,\n",
    "            'msg_minute_min': msg_minute_min_list,\n",
    "            'msg_minute_avg': msg_minute_avg_list,\n",
    "            'msg_minute_median': msg_minute_median_list,\n",
    "            'msg_minute_mode': msg_minute_mode_list,\n",
    "            'msg_log': msg_log_list,\n",
    "            'label': label_list\n",
    "        }\n",
    "    )\n",
    "    return msg_log_label_df\n",
    "\n",
    "# 计算特征函数\n",
    "def caculateFeature(log_df: pd.DataFrame, label_df: pd.DataFrame, word_list: list) -> pd.DataFrame:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    logger = get_logger('./user_data/logs/{}_info.log'.format(date.today()), 'info')\n",
    "    logger_error = get_logger('./user_data/logs/{}_error.log'.format(date.today()), 'error')\n",
    "\n",
    "    logger.info('开始拼接日志和标签数据')\n",
    "    log_df['label'] = ''\n",
    "    log_df['fault_time'] = ''\n",
    "    log_df = log_df[['sn', 'fault_time', 'msg', 'time', 'server_model', 'label']]\n",
    "\n",
    "    label_df['time'] = label_df['fault_time']\n",
    "    label_df['msg'] = ''\n",
    "    label_df['server_model'] = label_df['sn'].map(dict(zip(log_df['sn'], log_df['server_model'])))\n",
    "    label_df = label_df[['sn', 'fault_time', 'msg', 'time', 'server_model', 'label']]\n",
    "    log_label_df = pd.concat([log_df, label_df], axis=0).sort_values(by='time')\n",
    "#     log_label_df['fault_time'] = ''\n",
    "    log_label_df = log_label_df[['sn', 'fault_time', 'msg', 'time', 'server_model', 'label']]\n",
    "    logger.info('拼接日志和标签数据结束')\n",
    "\n",
    "    logger.info('开始匹配日志和标签')\n",
    "    logger.info('使用报错时间截断进行划分')\n",
    "    # 使用报错时间截断进行划分\n",
    "#     NearestTime_log_correspond_label_df, NearestTime_no_label_log_list = divideLogByNearestTime(log_label_df)\n",
    "#     NearestTime_log_correspond_label_df.to_csv('./user_data/tmp_data/NearestTime_log_correspond_label_df.csv', index = None)\n",
    "#     NearestTime_log_correspond_label_df = pd.read_csv('./user_data/tmp_data/NearestTime_log_correspond_label_df.csv')\n",
    "#     FaultTime_log_correspond_label_df, FaultTime_no_label_log_list = divideLogByFaultTime(log_label_df)\n",
    "#     FaultTime_log_correspond_label_df.to_csv('./user_data/tmp_data/FaultTime_log_correspond_label_df.csv', index = None)\n",
    "#     FaultTime_log_correspond_label_df = pd.read_csv('./user_data/tmp_data/FaultTime_log_correspond_label_df.csv')\n",
    "    logger.info('匹配日志和标签结束')\n",
    "\n",
    "    logger.info('开始计算统计特征')\n",
    "    # 使用报错时间截断进行划分\n",
    "    msg_log_label_df = calculateStatisticFeature(NearestTime_log_correspond_label_df)\n",
    "    logger.info('计算统计特征结束')\n",
    "\n",
    "    msg_log_list = list(msg_log_label_df['msg_log'])\n",
    "    label_list = list(msg_log_label_df['label'])\n",
    "\n",
    "    # 计算词频向量\n",
    "    logger.info('开始计算词频特征')\n",
    "    frequency_vector_list = []\n",
    "    tag = 0\n",
    "    for word in word_list:\n",
    "        if tag % 100 == 0:\n",
    "            print(tag, datetime.now())\n",
    "        pattern = re.compile(word)\n",
    "        frequency_vector = [len(re.findall(pattern, log)) for log in msg_log_list]\n",
    "        frequency_vector_list.append(frequency_vector)\n",
    "        tag += 1\n",
    "    logger.info('计算词频特征结束')\n",
    "\n",
    "    frequency_vector_df = pd.DataFrame(frequency_vector_list)\n",
    "    frequency_vector_df = frequency_vector_df.T\n",
    "    frequency_vector_df.columns = word_list\n",
    "    statistic_feature_list = list(msg_log_label_df.columns)[2:-2]\n",
    "    feature_df = frequency_vector_df\n",
    "    feature_df[statistic_feature_list] = msg_log_label_df[statistic_feature_list]\n",
    "\n",
    "    feature_df['label'] = label_list\n",
    "    feature_df[['sn', 'fault_time']] = msg_log_label_df[['sn', 'fault_time']]\n",
    "    logger.info('最后3列为: label, sn, fault_time, 其余列均为特征')\n",
    "    logger.info('数据条数: {}, 特征个数: {}'.format(feature_df.shape[0], feature_df.shape[1]-3))\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "# xgb模型参数\n",
    "xgb_params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'multi:softmax',  # 多分类问题\n",
    "    'num_class': 4,  # 类别数，与multi softmax并用\n",
    "    'gamma': 0.1,  # 用于控制是否后剪枝的参数，越大越保守，一般0.1 0.2的样子\n",
    "    'max_depth': 6,  # 构建树的深度，越大越容易过拟合\n",
    "    'lambda': 2,  # 控制模型复杂度的权重值的L2 正则化项参数，参数越大，模型越不容易过拟合\n",
    "    'subsample': 1,  # 随机采样训练样本\n",
    "    'colsample_bytree': 1,  # 这个参数默认为1，是每个叶子里面h的和至少是多少\n",
    "    # 对于正负样本不均衡时的0-1分类而言，假设h在0.01附近，min_child_weight为1\n",
    "    # 意味着叶子节点中最少需要包含100个样本。这个参数非常影响结果，\n",
    "    # 控制叶子节点中二阶导的和的最小值，该参数值越小，越容易过拟合\n",
    "    'silent': 0,  # 设置成1 则没有运行信息输入，最好是设置成0\n",
    "    'eta': 0.3,  # 如同学习率\n",
    "    'seed': 1000,\n",
    "    'nthread': 16,  # CPU线程数\n",
    "    # 'eval_metric':'auc'\n",
    "}\n",
    "\n",
    "# 指标评估\n",
    "def macro_f1(label_df: pd.DataFrame, prediction_df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    计算得分\n",
    "    :param label_df: [sn,fault_time,label]\n",
    "    :param prediction_df: [sn,fault_time,label]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    logger = get_logger('./user_data/logs/{}_info.log'.format(date.today()), 'info')\n",
    "    logger_error = get_logger('./user_data/logs/{}_error.log'.format(date.today()), 'error')\n",
    "\n",
    "    prediction_df.columns = ['sn', 'fault_time', 'prediction']\n",
    "    outcome_df = pd.merge(label_df, prediction_df ,how = 'left', on = ['sn', 'fault_time'])\n",
    "    weights = [5 / 11, 4 / 11, 1 / 11, 1 / 11]\n",
    "    macro_F1 = 0.\n",
    "    for i in range(len(weights)):\n",
    "        TP = len(outcome_df[(outcome_df['label'] == i) & (outcome_df['prediction'] == i)])\n",
    "        FP = len(outcome_df[(outcome_df['label'] != i) & (outcome_df['prediction'] == i)])\n",
    "        FN = len(outcome_df[(outcome_df['label'] == i) & (outcome_df['prediction'] != i)])\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        F1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        macro_F1 += weights[i] * F1\n",
    "        logger.info('Label {}:   Precision {: .2f}, Recall {: .2f}, F1 {: .2f}'.format(i, precision, recall, F1))\n",
    "    logger.info('macro_f1: {}\\n'.format(macro_F1))\n",
    "\n",
    "    return macro_F1\n",
    "\n",
    "# 模型训练函数\n",
    "def xgbTrain(feature_df: pd.DataFrame) -> xgb.XGBModel:\n",
    "    '''\n",
    "    feature_df: 要求最后3列为: label, sn, fault_time, 其余列均为特征\n",
    "    '''\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    logger = get_logger('./user_data/logs/{}_info.log'.format(date.today()), 'info')\n",
    "    logger_error = get_logger('./user_data/logs/{}_error.log'.format(date.today()), 'error')\n",
    "\n",
    "    feature_name_list = list(feature_df.columns)[0:-3]\n",
    "    feature = np.array(feature_df[feature_name_list])\n",
    "    label = np.array(feature_df['label'])\n",
    "    label_df = feature_df[['sn', 'fault_time', 'label']]\n",
    "    prediction_df = feature_df[['sn', 'fault_time']]\n",
    "\n",
    "    train_data = xgb.DMatrix(feature, label=label)\n",
    "    train_feature = xgb.DMatrix(feature)\n",
    "    logger.info('开始训练xgb模型')\n",
    "    xgb_model = xgb.train(xgb_params, train_data, num_boost_round=500)\n",
    "    logger.info('训练xgb模型结束')\n",
    "    # 训练集指标评估\n",
    "    prediction = xgb_model.predict(train_feature)\n",
    "    prediction_df['label'] = prediction\n",
    "    logger.info('训练集评估效果: ')\n",
    "    macro_f1(label_df, prediction_df)\n",
    "\n",
    "    return xgb_model\n",
    "\n",
    "# xgb模型预测函数\n",
    "def xgbPredict(model: xgb.XGBModel, feature_df: pd.DataFrame, label_df = None) -> pd.DataFrame:\n",
    "    '''\n",
    "        feature_df: 要求最后3列为: label, sn, fault_time, 其余列均为特征\n",
    "    '''\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    logger = get_logger('./user_data/logs/{}_info.log'.format(date.today()), 'info')\n",
    "    logger_error = get_logger('./user_data/logs/{}_error.log'.format(date.today()), 'error')\n",
    "\n",
    "    if label_df is None:\n",
    "        feature_name_list = list(feature_df.columns)[0:-3]\n",
    "        feature = np.array(feature_df[feature_name_list])\n",
    "        prediction_df = feature_df[['sn', 'fault_time']]\n",
    "\n",
    "        test_feature = xgb.DMatrix(feature)\n",
    "        logger.info('开始xgb模型预测')\n",
    "        prediction = model.predict(test_feature)\n",
    "        logger.info('xgb模型预测结束')\n",
    "        prediction_df['label'] = prediction\n",
    "        prediction_df['label'] = prediction_df['label'].apply(lambda x: int(x))\n",
    "\n",
    "    else:\n",
    "        feature_name_list = list(feature_df.columns)[0:-3]\n",
    "        feature = np.array(feature_df[feature_name_list])\n",
    "        prediction_df = feature_df[['sn', 'fault_time']]\n",
    "\n",
    "        test_feature = xgb.DMatrix(feature)\n",
    "        logger.info('开始xgb模型预测')\n",
    "        prediction = model.predict(test_feature)\n",
    "        logger.info('xgb模型预测结束')\n",
    "        # 测试集指标评估\n",
    "        prediction_df['label'] = prediction\n",
    "        prediction_df['label'] = prediction_df['label'].apply(lambda x: int(x))\n",
    "        logger.info('测试集评估效果: ')\n",
    "        macro_f1(label_df, prediction_df)\n",
    "\n",
    "    return prediction_df\n",
    "\n",
    "\n",
    "# xgb模型随机训练并投票预测\n",
    "def xgbRandomTrainPredict(train_feature_df: pd.DataFrame, test_feature_df: pd.DataFrame, label_df = None) -> pd.DataFrame:\n",
    "    ## 每个子模型样本均衡，利用投票规则生成最终预测\n",
    "    random.seed(0)\n",
    "    N = 50 # number of the models\n",
    "    num_sample = 700 # number of samples for each label\n",
    "\n",
    "    _label0_index_list = list(train_feature_df[train_feature_df['label'] == 0].index)\n",
    "    _label1_index_list = list(train_feature_df[train_feature_df['label'] == 1].index)\n",
    "    _label2_index_list = list(train_feature_df[train_feature_df['label'] == 2].index)\n",
    "    _label3_index_list = list(train_feature_df[train_feature_df['label'] == 3].index)\n",
    "    feature_name_list = list(train_feature_df.columns)[0:-3]\n",
    "    test_feature = np.array(test_feature_df[feature_name_list])\n",
    "    test_feature = xgb.DMatrix(test_feature)\n",
    "    prediction_df = test_feature_df[['sn', 'fault_time']]\n",
    "\n",
    "    for iter in np.arange(N):\n",
    "        idx_0 = random.sample(_label0_index_list, num_sample)\n",
    "        idx_1 = random.sample(_label1_index_list, num_sample)\n",
    "        idx_2 = random.sample(_label2_index_list, num_sample)\n",
    "        idx_3 = random.sample(_label3_index_list, num_sample)\n",
    "        idx = np.hstack((idx_0, idx_1, idx_2, idx_3))\n",
    "        random.shuffle(idx)\n",
    "        sub_train_feature_df = train_feature_df.loc[idx, :]\n",
    "        sub_train_feature = np.array(sub_train_feature_df[feature_name_list])\n",
    "        sub_train_label = np.array(sub_train_feature_df['label'])\n",
    "        sub_train_data = xgb.DMatrix(sub_train_feature,label = sub_train_label)\n",
    "\n",
    "        logger.info('开始第{}轮训练和预测'.format(iter))\n",
    "        sub_xgb_model = xgb.train(xgb_params, sub_train_data, num_boost_round=500)\n",
    "        sub_test_pred = sub_xgb_model.predict(test_feature)\n",
    "        if iter == 0:\n",
    "            val_pred = sub_test_pred\n",
    "        else:\n",
    "            val_pred = np.vstack((val_pred, sub_test_pred))\n",
    "        logger.info('第{}轮训练和预测结束'.format(iter))\n",
    "\n",
    "    # 训练集指标评估\n",
    "    final_pred = [np.argmax(np.bincount(val_pred[:, i].astype(int))) for i in np.arange(val_pred.shape[1])]\n",
    "    final_pred = np.array(final_pred).astype(int)\n",
    "    prediction_df['label'] = final_pred\n",
    "    logger.info('训练集评估效果: ')\n",
    "    macro_f1(label_df, prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56371e4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_feature_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 获取特征\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 计算特征\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# train_feature_df = caculateFeature(sel_log_df, train_label_df, word_list)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# train_feature_df.to_csv('./user_data/feature_data/nearesttime_train_feature_300_df.csv', index=None)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# train_feature_df = pd.read_csv('./user_data/feature_data/faulttime_train_feature_all_df.csv')\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# train_feature_df = pd.read_csv('./user_data/feature_data/nearesttime_train_feature_300_df.csv')\u001b[39;00m\n\u001b[0;32m     25\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m val_mask \u001b[38;5;241m=\u001b[39m [random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_feature_df\u001b[49m))]\n\u001b[0;32m     27\u001b[0m train_mask \u001b[38;5;241m=\u001b[39m [\u001b[38;5;129;01mnot\u001b[39;00m xx \u001b[38;5;28;01mfor\u001b[39;00m xx \u001b[38;5;129;01min\u001b[39;00m val_mask]\n\u001b[0;32m     28\u001b[0m temp_feature_df \u001b[38;5;241m=\u001b[39m train_feature_df[train_mask]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_feature_df' is not defined"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.dirname(sys.path[0]))\n",
    "# 忽略warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = get_logger('./user_data/logs/{}_info.log'.format(date.today()), 'info')\n",
    "logger_error = get_logger('./user_data/logs/{}_error.log'.format(date.today()), 'error')\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "# 读取sel日志数据\n",
    "sel_log_df = pd.read_csv('./data/preliminary_train/preliminary_sel_log_dataset.csv').drop_duplicates()\n",
    "# 读取训练标签数据：有重复数据！\n",
    "train_label1 = pd.read_csv('./data/preliminary_train/preliminary_train_label_dataset.csv')\n",
    "train_label2 = pd.read_csv('./data/preliminary_train/preliminary_train_label_dataset_s.csv')\n",
    "train_label_df = pd.concat([train_label1,train_label2],axis=0).drop_duplicates()\n",
    "# 读取词列表\n",
    "important_word_list = list(pd.read_csv('./user_data/words/important_word_df.csv')['word'])\n",
    "word_list = important_word_list\n",
    "\n",
    "# 获取特征\n",
    "# 计算特征\n",
    "# train_feature_df = caculateFeature(sel_log_df, train_label_df, word_list)\n",
    "# train_feature_df.to_csv('./user_data/feature_data/nearesttime_train_feature_300_df.csv', index=None)\n",
    "train_feature_df = pd.read_csv('./user_data/feature_data/faulttime_train_feature_300_df.csv')\n",
    "# train_feature_df = pd.read_csv('./user_data/feature_data/nearesttime_train_feature_300_df.csv')\n",
    "random.seed(0)\n",
    "val_mask = [random.random() < 0.3 for _ in range(len(train_feature_df))]\n",
    "train_mask = [not xx for xx in val_mask]\n",
    "temp_feature_df = train_feature_df[train_mask]\n",
    "val_feature_df = train_feature_df[val_mask]\n",
    "val_feature_df = val_feature_df[(val_feature_df['msg_1_cnt'] == 0) & (val_feature_df['msg_2_cnt'] == 0) & (val_feature_df['msg_3_cnt'] == 0) & (val_feature_df['msg_4_cnt'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_feature_df = val_feature_df[(val_feature_df['msg_1_cnt'] == 0) & (val_feature_df['msg_2_cnt'] == 0) & (val_feature_df['msg_3_cnt'] == 0) & (val_feature_df['msg_4_cnt'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb8eea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取sel日志数据\n",
    "sel_log_df = pd.read_csv('./data/preliminary_train/preliminary_sel_log_dataset.csv').drop_duplicates()\n",
    "# 读取训练标签数据：有重复数据！\n",
    "train_label1 = pd.read_csv('./data/preliminary_train/preliminary_train_label_dataset.csv')\n",
    "train_label2 = pd.read_csv('./data/preliminary_train/preliminary_train_label_dataset_s.csv')\n",
    "train_label_df = pd.concat([train_label1,train_label2],axis=0).drop_duplicates()\n",
    "\n",
    "log_df = sel_log_df\n",
    "label_df = train_label_df\n",
    "log_df['label'] = ''\n",
    "log_df['fault_time'] = ''\n",
    "log_df = log_df[['sn', 'fault_time', 'msg', 'time', 'server_model', 'label']]\n",
    "\n",
    "label_df['time'] = label_df['fault_time']\n",
    "label_df['msg'] = ''\n",
    "label_df['server_model'] = label_df['sn'].map(dict(zip(log_df['sn'], log_df['server_model'])))\n",
    "label_df = label_df[['sn', 'fault_time', 'msg', 'time', 'server_model', 'label']]\n",
    "log_label_df = pd.concat([log_df, label_df], axis=0).sort_values(by='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4378ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = log_label_df[log_label_df['msg'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c253f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     NearestTime_log_correspond_label_df, NearestTime_no_label_log_list = divideLogByNearestTime(log_label_df)\n",
    "#     NearestTime_log_correspond_label_df.to_csv('./user_data/tmp_data/NearestTime_log_correspond_label_df.csv', index = None)\n",
    "NearestTime_log_correspond_label_df = pd.read_csv('./user_data/tmp_data/NearestTime_log_correspond_label_df.csv')\n",
    "#     FaultTime_log_correspond_label_df, FaultTime_no_label_log_list = divideLogByFaultTime(log_label_df)\n",
    "#     FaultTime_log_correspond_label_df.to_csv('./user_data/tmp_data/FaultTime_log_correspond_label_df.csv', index = None)\n",
    "FaultTime_log_correspond_label_df = pd.read_csv('./user_data/tmp_data/FaultTime_log_correspond_label_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e04c2199",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_nearesttime_df = NearestTime_log_correspond_label_df[NearestTime_log_correspond_label_df['msg'].isna()]\n",
    "na_faulttime_df = FaultTime_log_correspond_label_df[FaultTime_log_correspond_label_df['msg'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3fd9070a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sn</th>\n",
       "      <th>fault_time</th>\n",
       "      <th>msg</th>\n",
       "      <th>time</th>\n",
       "      <th>server_model</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>476571</th>\n",
       "      <td>SERVER_10108</td>\n",
       "      <td>2020-05-02 14:47:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-02 14:47:00</td>\n",
       "      <td>SM35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476572</th>\n",
       "      <td>SERVER_10109</td>\n",
       "      <td>2020-08-11 15:35:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-11 15:35:00</td>\n",
       "      <td>SM35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476573</th>\n",
       "      <td>SERVER_10110</td>\n",
       "      <td>2020-05-17 18:22:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-17 18:22:00</td>\n",
       "      <td>SM35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476574</th>\n",
       "      <td>SERVER_10110</td>\n",
       "      <td>2020-05-17 18:24:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-17 18:24:00</td>\n",
       "      <td>SM35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476575</th>\n",
       "      <td>SERVER_10111</td>\n",
       "      <td>2020-05-06 15:51:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-06 15:51:00</td>\n",
       "      <td>SM35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477291</th>\n",
       "      <td>SERVER_9715</td>\n",
       "      <td>2020-03-03 17:58:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-03 17:58:00</td>\n",
       "      <td>SM51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477292</th>\n",
       "      <td>SERVER_9780</td>\n",
       "      <td>2020-04-15 05:47:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-04-15 05:47:00</td>\n",
       "      <td>SM56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477293</th>\n",
       "      <td>SERVER_9864</td>\n",
       "      <td>2020-04-05 10:43:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-04-05 10:43:00</td>\n",
       "      <td>SM56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477294</th>\n",
       "      <td>SERVER_9879</td>\n",
       "      <td>2020-08-06 05:41:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-06 05:41:00</td>\n",
       "      <td>SM56</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477295</th>\n",
       "      <td>SERVER_998</td>\n",
       "      <td>2020-09-05 14:07:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-05 14:07:00</td>\n",
       "      <td>SM15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>725 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  sn           fault_time  msg                 time  \\\n",
       "476571  SERVER_10108  2020-05-02 14:47:00  NaN  2020-05-02 14:47:00   \n",
       "476572  SERVER_10109  2020-08-11 15:35:00  NaN  2020-08-11 15:35:00   \n",
       "476573  SERVER_10110  2020-05-17 18:22:00  NaN  2020-05-17 18:22:00   \n",
       "476574  SERVER_10110  2020-05-17 18:24:00  NaN  2020-05-17 18:24:00   \n",
       "476575  SERVER_10111  2020-05-06 15:51:00  NaN  2020-05-06 15:51:00   \n",
       "...              ...                  ...  ...                  ...   \n",
       "477291   SERVER_9715  2020-03-03 17:58:00  NaN  2020-03-03 17:58:00   \n",
       "477292   SERVER_9780  2020-04-15 05:47:00  NaN  2020-04-15 05:47:00   \n",
       "477293   SERVER_9864  2020-04-05 10:43:00  NaN  2020-04-05 10:43:00   \n",
       "477294   SERVER_9879  2020-08-06 05:41:00  NaN  2020-08-06 05:41:00   \n",
       "477295    SERVER_998  2020-09-05 14:07:00  NaN  2020-09-05 14:07:00   \n",
       "\n",
       "       server_model  label  \n",
       "476571         SM35      2  \n",
       "476572         SM35      2  \n",
       "476573         SM35      2  \n",
       "476574         SM35      2  \n",
       "476575         SM35      2  \n",
       "...             ...    ...  \n",
       "477291         SM51      2  \n",
       "477292         SM56      1  \n",
       "477293         SM56      1  \n",
       "477294         SM56      2  \n",
       "477295         SM15      3  \n",
       "\n",
       "[725 rows x 6 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_nearesttime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6727c69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sn</th>\n",
       "      <th>fault_time</th>\n",
       "      <th>msg</th>\n",
       "      <th>time</th>\n",
       "      <th>server_model</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>SERVER_10108</td>\n",
       "      <td>2020-05-02 14:47:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-02 14:47:00</td>\n",
       "      <td>SM35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082</th>\n",
       "      <td>SERVER_10110</td>\n",
       "      <td>2020-05-17 18:22:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-17 18:22:00</td>\n",
       "      <td>SM35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>SERVER_10110</td>\n",
       "      <td>2020-05-17 18:24:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-17 18:24:00</td>\n",
       "      <td>SM35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2108</th>\n",
       "      <td>SERVER_10111</td>\n",
       "      <td>2020-05-06 15:51:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-06 15:51:00</td>\n",
       "      <td>SM35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>SERVER_10114</td>\n",
       "      <td>2020-05-03 12:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-03 12:15:00</td>\n",
       "      <td>SM35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480332</th>\n",
       "      <td>SERVER_9715</td>\n",
       "      <td>2020-03-03 17:58:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-03-03 17:58:00</td>\n",
       "      <td>SM51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480983</th>\n",
       "      <td>SERVER_9773</td>\n",
       "      <td>2020-09-13 00:58:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-13 00:58:00</td>\n",
       "      <td>SM56</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481402</th>\n",
       "      <td>SERVER_9855</td>\n",
       "      <td>2020-05-24 02:13:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-24 02:13:00</td>\n",
       "      <td>SM56</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481771</th>\n",
       "      <td>SERVER_9870</td>\n",
       "      <td>2020-05-21 22:57:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-21 22:57:00</td>\n",
       "      <td>SM56</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482261</th>\n",
       "      <td>SERVER_998</td>\n",
       "      <td>2020-09-05 14:07:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-09-05 14:07:00</td>\n",
       "      <td>SM15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>590 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  sn           fault_time  msg                 time  \\\n",
       "2030    SERVER_10108  2020-05-02 14:47:00  NaN  2020-05-02 14:47:00   \n",
       "2082    SERVER_10110  2020-05-17 18:22:00  NaN  2020-05-17 18:22:00   \n",
       "2083    SERVER_10110  2020-05-17 18:24:00  NaN  2020-05-17 18:24:00   \n",
       "2108    SERVER_10111  2020-05-06 15:51:00  NaN  2020-05-06 15:51:00   \n",
       "2125    SERVER_10114  2020-05-03 12:15:00  NaN  2020-05-03 12:15:00   \n",
       "...              ...                  ...  ...                  ...   \n",
       "480332   SERVER_9715  2020-03-03 17:58:00  NaN  2020-03-03 17:58:00   \n",
       "480983   SERVER_9773  2020-09-13 00:58:00  NaN  2020-09-13 00:58:00   \n",
       "481402   SERVER_9855  2020-05-24 02:13:00  NaN  2020-05-24 02:13:00   \n",
       "481771   SERVER_9870  2020-05-21 22:57:00  NaN  2020-05-21 22:57:00   \n",
       "482261    SERVER_998  2020-09-05 14:07:00  NaN  2020-09-05 14:07:00   \n",
       "\n",
       "       server_model  label  \n",
       "2030           SM35      2  \n",
       "2082           SM35      2  \n",
       "2083           SM35      2  \n",
       "2108           SM35      2  \n",
       "2125           SM35      2  \n",
       "...             ...    ...  \n",
       "480332         SM51      2  \n",
       "480983         SM56      2  \n",
       "481402         SM56      2  \n",
       "481771         SM56      2  \n",
       "482261         SM15      3  \n",
       "\n",
       "[590 rows x 6 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_faulttime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "94a79055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0      1\n",
       "1     71\n",
       "2    438\n",
       "3     80\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_faulttime_df.groupby('label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31b3fcac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0      3\n",
       "1    159\n",
       "2    472\n",
       "3     91\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_nearesttime_df.groupby('label').size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Log_diagosis_python",
   "language": "python",
   "name": "log_diagosis_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
