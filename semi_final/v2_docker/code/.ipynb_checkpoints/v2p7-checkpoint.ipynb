{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ac94bf",
   "metadata": {},
   "source": [
    "# 剔除无日志标签效果\n",
    "### 最近邻时间\n",
    "\n",
    "### 报错时间 - 不含na直接\n",
    "0.6648\n",
    "### 报错时间 - 不含投票\n",
    "0.6655\n",
    "### 报错时间 - 取出na后直接\n",
    "0.6359\n",
    "### 报错时间 - 取出na后投票\n",
    "0.6219\n",
    "\n",
    "### 最近邻时间 - 不含直接\n",
    "0.6559\n",
    "### 最近邻时间 - 不含投票\n",
    "0.6745\n",
    "### 最近邻时间 - 取出na后直接\n",
    "0.6352\n",
    "### 最近邻时间 - 取出na后投票\n",
    "0.6295\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d587f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "from logging import handlers\n",
    "from datetime import datetime, date\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import traceback\n",
    "import re\n",
    "import random\n",
    "\n",
    "\n",
    "## 日志格式设置\n",
    "# 日志级别关系映射\n",
    "level_relations = {\n",
    "    'debug': logging.DEBUG,\n",
    "    'info': logging.INFO,\n",
    "    'warning': logging.WARNING,\n",
    "    'error': logging.ERROR,\n",
    "    'crit': logging.CRITICAL\n",
    "}\n",
    "def get_logger(filename, level='info'):\n",
    "    # 创建日志对象\n",
    "    log = logging.getLogger(filename)\n",
    "    # 设置日志级别\n",
    "    log.setLevel(level_relations.get(level))\n",
    "    # 日志输出格式\n",
    "    fmt = logging.Formatter('%(asctime)s %(thread)d %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s')\n",
    "    # 输出到控制台\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(fmt)\n",
    "    # 输出到文件\n",
    "    # 日志文件按天进行保存，每天一个日志文件\n",
    "    file_handler = handlers.TimedRotatingFileHandler(filename=filename, when='D', backupCount=1, encoding='utf-8')\n",
    "    # 按照大小自动分割日志文件，一旦达到指定的大小重新生成文件\n",
    "    # file_handler = handlers.RotatingFileHandler(filename=filename, maxBytes=1*1024*1024*1024, backupCount=1, encoding='utf-8')\n",
    "    file_handler.setFormatter(fmt)\n",
    "    if not log.handlers:\n",
    "        log.addHandler(console_handler)\n",
    "        log.addHandler(file_handler)\n",
    "    return log\n",
    "\n",
    "# sn分组后，本次报错和上次报错之间的日志匹配到本次报错\n",
    "def divideLogByFaultTime(log_label_df: pd.DataFrame):\n",
    "    log_correspond_label_df = pd.DataFrame(columns=['sn', 'fault_time', 'msg', 'time', 'server_model', 'label'])\n",
    "    no_label_log_list = []\n",
    "    log_label_df = log_label_df.reset_index(drop=True)\n",
    "\n",
    "    for sn, log in log_label_df.groupby('sn'):\n",
    "        if len(log[log['label'] != '']) == 0:\n",
    "            no_label_log_list.append(log)\n",
    "        elif len(log[log['label'] != '']) == 1:\n",
    "            msg_df = log[log['label'] == '']\n",
    "            msg_df['label'] = log[log['label'] != '']['label'].iloc[0]\n",
    "            msg_df['fault_time'] = log[log['label'] != '']['time'].iloc[0]\n",
    "            log_correspond_label_df = pd.concat([log_correspond_label_df, msg_df])\n",
    "        else:\n",
    "            # 使用index的顺序取数时，要注意index必须按所需的顺序排列\n",
    "            cutoff_index = [-1] + log.loc[log['label'] != ''].index.tolist() + [log.index.tolist()[-1] + 1]\n",
    "            for kth in range(len(cutoff_index) - 1):\n",
    "                temp_log = log.loc[(log.index <= cutoff_index[kth + 1]) & (log.index > cutoff_index[kth])]\n",
    "                if len(temp_log) > 0:\n",
    "                    if len(temp_log[temp_log['label'] != '']) == 0:\n",
    "                        no_label_log_list.append(temp_log)\n",
    "                    # 只有标签，没有日志的数据，把标签的部分数据直接作为日志\n",
    "                    elif len(temp_log) == 1:\n",
    "                        msg_df = temp_log\n",
    "                        msg_df['fault_time'] = temp_log[temp_log['label'] != '']['time'].iloc[0]\n",
    "                        log_correspond_label_df = pd.concat([log_correspond_label_df, msg_df])\n",
    "                    else:\n",
    "                        msg_df = temp_log[temp_log['label'] == '']\n",
    "                        msg_df['label'] = temp_log[temp_log['label'] != '']['label'].iloc[0]\n",
    "                        msg_df['fault_time'] = temp_log[temp_log['label'] != '']['time'].iloc[0]\n",
    "                        log_correspond_label_df = pd.concat([log_correspond_label_df, msg_df])\n",
    "    return log_correspond_label_df, no_label_log_list\n",
    "\n",
    "# sn分组后，按照最近邻+时间间隔划分日志数据\n",
    "def divideLogByNearestTime(log_label_df: pd.DataFrame):\n",
    "    log_correspond_label_df = pd.DataFrame(columns=['sn', 'fault_time', 'msg', 'time', 'server_model', 'label'])\n",
    "    origin_label_df = log_label_df[log_label_df['fault_time'] != '']\n",
    "    no_label_log_list = []\n",
    "    cutoff = 10 * 3600\n",
    "\n",
    "    for sn, log in log_label_df.groupby('sn'):\n",
    "        if len(log[log['label'] != '']) == 0:\n",
    "            no_label_log_list.append(log)\n",
    "        elif len(log[log['label'] != '']) == 1:\n",
    "            msg_df = log[log['label'] == '']\n",
    "            if len(msg_df) > 0:\n",
    "                msg_df['label'] = log[log['label'] != '']['label'].iloc[0]\n",
    "                msg_df['fault_time'] = log[log['label'] != '']['time'].iloc[0]\n",
    "                log_correspond_label_df = pd.concat([log_correspond_label_df, msg_df])\n",
    "        else:\n",
    "            lable_df = log[log['label'] != '']\n",
    "            msg_df = log[log['label'] == '']\n",
    "            for msg_item in msg_df.iterrows():\n",
    "                previous_delta_time = 1000 * 24 * 3600\n",
    "                for lable_item in lable_df.iterrows():\n",
    "                    now_delta_time = abs(datetime.strptime(lable_item[1]['time'],'%Y-%m-%d %H:%M:%S'\n",
    "                        ) - datetime.strptime(msg_item[1]['time'],'%Y-%m-%d %H:%M:%S'))\n",
    "                    if now_delta_time.days * 24 * 3600 + now_delta_time.seconds < previous_delta_time:\n",
    "                        previous_delta_time = now_delta_time.days * 24 * 3600 + now_delta_time.seconds\n",
    "                        final_lable = lable_item[1]\n",
    "                        if previous_delta_time < cutoff:\n",
    "                            msg_item[1]['fault_time'] = lable_item[1]['time']\n",
    "                            msg_item[1]['label'] = lable_item[1]['label']\n",
    "            log_correspond_label_df = pd.concat([log_correspond_label_df, msg_df]) \n",
    "    log_correspond_label_df = log_correspond_label_df[log_correspond_label_df['label'] != '']\n",
    "    # 找出没有匹配到日志的标签并将其添加到 日志标签映射表 中\n",
    "    temp_df = pd.concat([log_correspond_label_df, origin_label_df])\n",
    "    sn_list = []\n",
    "    fault_time_list = []\n",
    "    msg_list = []\n",
    "    time_list = []\n",
    "    server_model_list = []\n",
    "    label_list = []\n",
    "    for g in temp_df.groupby(['sn', 'fault_time', 'label']):\n",
    "        if len(g[1]) == 1:\n",
    "            sn_list.append(g[0][0])\n",
    "            fault_time_list.append(g[0][1])\n",
    "            msg_list.append('')\n",
    "            time_list.append(g[1]['time'].iloc[0])\n",
    "            server_model_list.append(g[1]['server_model'].iloc[0])\n",
    "            label_list.append(g[0][2])\n",
    "    no_log_label_df  = pd.DataFrame({\n",
    "        'sn': sn_list,\n",
    "        'fault_time': fault_time_list,\n",
    "        'msg': msg_list,\n",
    "        'time': time_list,\n",
    "        'server_model': server_model_list,\n",
    "        'label': label_list\n",
    "    })\n",
    "    log_correspond_label_df = pd.concat([log_correspond_label_df, no_log_label_df])\n",
    "    return log_correspond_label_df, no_label_log_list\n",
    "\n",
    "# 计算统计特征\n",
    "def calculateStatisticFeature(log_correspond_label_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    use_log_label_df = log_correspond_label_df\n",
    "\n",
    "    use_log_label_df['msg_hour'] = use_log_label_df['time'].apply(\n",
    "        lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").hour)\n",
    "    use_log_label_df['msg_minute'] = use_log_label_df['time'].apply(\n",
    "        lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").minute)\n",
    "    use_log_label_df['fault_hour'] = use_log_label_df['fault_time'].apply(\n",
    "        lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").hour)\n",
    "    use_log_label_df['fault_minute'] = use_log_label_df['fault_time'].apply(\n",
    "        lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").minute)\n",
    "\n",
    "    # 0408新增\n",
    "    # 最近一次日志时间距报错时间间隔，单位秒\n",
    "    nearest_msg_fault_time_delta_list = []\n",
    "    # 日志不去重时长度1,2,3,4日志数量统计\n",
    "    all_msg_1_cnt_list = []\n",
    "    all_msg_2_cnt_list = []\n",
    "    all_msg_3_cnt_list = []\n",
    "    all_msg_4_cnt_list = []\n",
    "\n",
    "    fault_minute_list = []\n",
    "    msg_1_cnt_list = []\n",
    "    msg_2_cnt_list = []\n",
    "    msg_3_cnt_list = []\n",
    "    msg_4_cnt_list = []\n",
    "    msg_hour_max_list = []\n",
    "    msg_hour_min_list = []\n",
    "    msg_hour_avg_list = []\n",
    "    msg_hour_median_list = []\n",
    "    msg_hour_mode_list = []\n",
    "    msg_minute_max_list = []\n",
    "    msg_minute_min_list = []\n",
    "    msg_minute_avg_list = []\n",
    "    msg_minute_median_list = []\n",
    "    msg_minute_mode_list = []\n",
    "\n",
    "    sn_list = []\n",
    "    server_model_list = []\n",
    "    msg_log_list = []\n",
    "    msg_cnt_list = []\n",
    "    fault_hour_list = []\n",
    "    label_list = []\n",
    "    fault_time_list = []\n",
    "    for msg_log_df in use_log_label_df.groupby(['sn', 'fault_time', 'label']):\n",
    "        msg_log_str = ''\n",
    "        all_msg_1_cnt = 0\n",
    "        all_msg_2_cnt = 0\n",
    "        all_msg_3_cnt = 0\n",
    "        all_msg_4_cnt = 0\n",
    "        msg_1_cnt = 0\n",
    "        msg_2_cnt = 0\n",
    "        msg_3_cnt = 0\n",
    "        msg_4_cnt = 0\n",
    "        for info in msg_log_df[1]['msg']:\n",
    "            if info == info:\n",
    "                if len(info.split('|')) == 1:\n",
    "                    all_msg_1_cnt += 1\n",
    "                elif len(info.split('|')) == 2:\n",
    "                    all_msg_2_cnt += 1\n",
    "                elif len(info.split('|')) == 3:\n",
    "                    all_msg_3_cnt += 1\n",
    "                else:\n",
    "                    all_msg_4_cnt += 1\n",
    "        for info in msg_log_df[1]['msg'].drop_duplicates():\n",
    "            if info == info:\n",
    "                msg_log_str = msg_log_str + info.lower() + '.'\n",
    "                if len(info.split('|')) == 1:\n",
    "                    msg_1_cnt += 1\n",
    "                elif len(info.split('|')) == 2:\n",
    "                    msg_2_cnt += 1\n",
    "                elif len(info.split('|')) == 3:\n",
    "                    msg_3_cnt += 1\n",
    "                else:\n",
    "                    msg_4_cnt += 1\n",
    "        nearest_msg_fault_time_delta = abs(datetime.strptime(msg_log_df[1].iloc[-1]['time'], '%Y-%m-%d %H:%M:%S'\n",
    "                                                             ) - datetime.strptime(msg_log_df[0][1],\n",
    "                                                                                   '%Y-%m-%d %H:%M:%S'))\n",
    "        nearest_msg_fault_time_delta = nearest_msg_fault_time_delta.days * 24 * 3600 + nearest_msg_fault_time_delta.seconds\n",
    "        sm = int(msg_log_df[1].iloc[0]['server_model'][2:])\n",
    "\n",
    "        sn_list.append(msg_log_df[0][0])\n",
    "        fault_time_list.append(msg_log_df[0][1])\n",
    "        label_list.append(msg_log_df[0][2])\n",
    "\n",
    "        nearest_msg_fault_time_delta_list.append(nearest_msg_fault_time_delta)\n",
    "        server_model_list.append(sm)\n",
    "        msg_log_list.append(msg_log_str)\n",
    "        msg_cnt_list.append(len(msg_log_df[1]))\n",
    "\n",
    "        fault_hour_list.append(msg_log_df[1].iloc[0]['fault_hour'])\n",
    "        fault_minute_list.append(msg_log_df[1].iloc[0]['fault_minute'])\n",
    "\n",
    "        all_msg_1_cnt_list.append(all_msg_1_cnt)\n",
    "        all_msg_2_cnt_list.append(all_msg_2_cnt)\n",
    "        all_msg_3_cnt_list.append(all_msg_3_cnt)\n",
    "        all_msg_4_cnt_list.append(all_msg_4_cnt)\n",
    "\n",
    "        msg_1_cnt_list.append(msg_1_cnt)\n",
    "        msg_2_cnt_list.append(msg_2_cnt)\n",
    "        msg_3_cnt_list.append(msg_3_cnt)\n",
    "        msg_4_cnt_list.append(msg_4_cnt)\n",
    "\n",
    "        msg_hour_max_list.append(msg_log_df[1]['msg_hour'].max())\n",
    "        msg_hour_min_list.append(msg_log_df[1]['msg_hour'].min())\n",
    "        msg_hour_avg_list.append(msg_log_df[1]['msg_hour'].mean())\n",
    "        msg_hour_median_list.append(msg_log_df[1]['msg_hour'].median())\n",
    "        msg_hour_mode_list.append(msg_log_df[1]['msg_hour'].mode()[0])\n",
    "\n",
    "        msg_minute_max_list.append(msg_log_df[1]['msg_minute'].max())\n",
    "        msg_minute_min_list.append(msg_log_df[1]['msg_minute'].min())\n",
    "        msg_minute_avg_list.append(msg_log_df[1]['msg_minute'].mean())\n",
    "        msg_minute_median_list.append(msg_log_df[1]['msg_minute'].median())\n",
    "        msg_minute_mode_list.append(msg_log_df[1]['msg_minute'].mode()[0])\n",
    "\n",
    "    msg_log_label_df = pd.DataFrame(\n",
    "        {\n",
    "            'sn': sn_list,\n",
    "            'fault_time': fault_time_list,\n",
    "            'server_model': server_model_list,\n",
    "            'msg_cnt': msg_cnt_list,\n",
    "            'fault_hour': fault_hour_list,\n",
    "            'fault_minute': fault_minute_list,\n",
    "            'nearest_msg_fault_time_delta': nearest_msg_fault_time_delta_list,\n",
    "            'all_msg_1_cnt': all_msg_1_cnt_list,\n",
    "            'all_msg_2_cnt': all_msg_2_cnt_list,\n",
    "            'all_msg_3_cnt': all_msg_3_cnt_list,\n",
    "            'all_msg_4_cnt': all_msg_4_cnt_list,\n",
    "            'msg_1_cnt': msg_1_cnt_list,\n",
    "            'msg_2_cnt': msg_2_cnt_list,\n",
    "            'msg_3_cnt': msg_3_cnt_list,\n",
    "            'msg_4_cnt': msg_4_cnt_list,\n",
    "            'msg_hour_max': msg_hour_max_list,\n",
    "            'msg_hour_min': msg_hour_min_list,\n",
    "            'msg_hour_avg': msg_hour_avg_list,\n",
    "            'msg_hour_median': msg_hour_median_list,\n",
    "            'msg_hour_mode': msg_hour_mode_list,\n",
    "            'msg_minute_max': msg_minute_max_list,\n",
    "            'msg_minute_min': msg_minute_min_list,\n",
    "            'msg_minute_avg': msg_minute_avg_list,\n",
    "            'msg_minute_median': msg_minute_median_list,\n",
    "            'msg_minute_mode': msg_minute_mode_list,\n",
    "            'msg_log': msg_log_list,\n",
    "            'label': label_list\n",
    "        }\n",
    "    )\n",
    "    return msg_log_label_df\n",
    "\n",
    "# 计算特征函数\n",
    "def caculateFeature(log_df: pd.DataFrame, label_df: pd.DataFrame, word_list: list) -> pd.DataFrame:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    logger = get_logger('./user_data/logs/{}_info.log'.format(date.today()), 'info')\n",
    "    logger_error = get_logger('./user_data/logs/{}_error.log'.format(date.today()), 'error')\n",
    "\n",
    "    logger.info('开始拼接日志和标签数据')\n",
    "    log_df['label'] = ''\n",
    "    log_df['fault_time'] = ''\n",
    "    log_df = log_df[['sn', 'fault_time', 'msg', 'time', 'server_model', 'label']]\n",
    "\n",
    "    label_df['time'] = label_df['fault_time']\n",
    "    label_df['msg'] = ''\n",
    "    label_df['server_model'] = label_df['sn'].map(dict(zip(log_df['sn'], log_df['server_model'])))\n",
    "    label_df = label_df[['sn', 'fault_time', 'msg', 'time', 'server_model', 'label']]\n",
    "    log_label_df = pd.concat([log_df, label_df], axis=0).sort_values(by='time')\n",
    "#     log_label_df['fault_time'] = ''\n",
    "    log_label_df = log_label_df[['sn', 'fault_time', 'msg', 'time', 'server_model', 'label']]\n",
    "    logger.info('拼接日志和标签数据结束')\n",
    "\n",
    "    logger.info('开始匹配日志和标签')\n",
    "    logger.info('使用报错时间截断进行划分')\n",
    "    # 使用报错时间截断进行划分\n",
    "#     NearestTime_log_correspond_label_df, NearestTime_no_label_log_list = divideLogByNearestTime(log_label_df)\n",
    "#     NearestTime_log_correspond_label_df.to_csv('./user_data/tmp_data/NearestTime_log_correspond_label_df.csv', index = None)\n",
    "    NearestTime_log_correspond_label_df = pd.read_csv('./user_data/tmp_data/NearestTime_log_correspond_label_df.csv')\n",
    "#     FaultTime_log_correspond_label_df, FaultTime_no_label_log_list = divideLogByFaultTime(log_label_df)\n",
    "#     FaultTime_log_correspond_label_df.to_csv('./user_data/tmp_data/FaultTime_log_correspond_label_df.csv', index = None)\n",
    "    FaultTime_log_correspond_label_df = pd.read_csv('./user_data/tmp_data/FaultTime_log_correspond_label_df.csv')\n",
    "    \n",
    "#     NearestTime_log_correspond_label_df = NearestTime_log_correspond_label_df[NearestTime_log_correspond_label_df['msg'].notna()]\n",
    "#     FaultTime_log_correspond_label_df = FaultTime_log_correspond_label_df[FaultTime_log_correspond_label_df['msg'].notna()]\n",
    "    logger.info('匹配日志和标签结束')\n",
    "\n",
    "    logger.info('开始计算统计特征')\n",
    "    # 使用报错时间截断进行划分\n",
    "    msg_log_label_df = calculateStatisticFeature(NearestTime_log_correspond_label_df)\n",
    "    logger.info('计算统计特征结束')\n",
    "\n",
    "    msg_log_list = list(msg_log_label_df['msg_log'])\n",
    "    label_list = list(msg_log_label_df['label'])\n",
    "\n",
    "    # 计算词频向量\n",
    "    logger.info('开始计算词频特征')\n",
    "    frequency_vector_list = []\n",
    "    tag = 0\n",
    "    for word in word_list:\n",
    "        if tag % 100 == 0:\n",
    "            print(tag, datetime.now())\n",
    "        pattern = re.compile(word)\n",
    "        frequency_vector = [len(re.findall(pattern, log)) for log in msg_log_list]\n",
    "        frequency_vector_list.append(frequency_vector)\n",
    "        tag += 1\n",
    "    logger.info('计算词频特征结束')\n",
    "\n",
    "    frequency_vector_df = pd.DataFrame(frequency_vector_list)\n",
    "    frequency_vector_df = frequency_vector_df.T\n",
    "    frequency_vector_df.columns = word_list\n",
    "    statistic_feature_list = list(msg_log_label_df.columns)[2:-2]\n",
    "    feature_df = frequency_vector_df\n",
    "    feature_df[statistic_feature_list] = msg_log_label_df[statistic_feature_list]\n",
    "\n",
    "    feature_df['label'] = label_list\n",
    "    feature_df[['sn', 'fault_time']] = msg_log_label_df[['sn', 'fault_time']]\n",
    "    logger.info('最后3列为: label, sn, fault_time, 其余列均为特征')\n",
    "    logger.info('数据条数: {}, 特征个数: {}'.format(feature_df.shape[0], feature_df.shape[1]-3))\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "# xgb模型参数\n",
    "xgb_params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'multi:softmax',  # 多分类问题\n",
    "    'num_class': 4,  # 类别数，与multi softmax并用\n",
    "    'gamma': 0.1,  # 用于控制是否后剪枝的参数，越大越保守，一般0.1 0.2的样子\n",
    "    'max_depth': 6,  # 构建树的深度，越大越容易过拟合\n",
    "    'lambda': 2,  # 控制模型复杂度的权重值的L2 正则化项参数，参数越大，模型越不容易过拟合\n",
    "    'subsample': 1,  # 随机采样训练样本\n",
    "    'colsample_bytree': 1,  # 这个参数默认为1，是每个叶子里面h的和至少是多少\n",
    "    # 对于正负样本不均衡时的0-1分类而言，假设h在0.01附近，min_child_weight为1\n",
    "    # 意味着叶子节点中最少需要包含100个样本。这个参数非常影响结果，\n",
    "    # 控制叶子节点中二阶导的和的最小值，该参数值越小，越容易过拟合\n",
    "    'silent': 0,  # 设置成1 则没有运行信息输入，最好是设置成0\n",
    "    'eta': 0.3,  # 如同学习率\n",
    "    'seed': 1000,\n",
    "    'nthread': 16,  # CPU线程数\n",
    "    # 'eval_metric':'auc'\n",
    "}\n",
    "\n",
    "# 指标评估\n",
    "def macro_f1(label_df: pd.DataFrame, prediction_df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    计算得分\n",
    "    :param label_df: [sn,fault_time,label]\n",
    "    :param prediction_df: [sn,fault_time,label]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    logger = get_logger('./user_data/logs/{}_info.log'.format(date.today()), 'info')\n",
    "    logger_error = get_logger('./user_data/logs/{}_error.log'.format(date.today()), 'error')\n",
    "\n",
    "    prediction_df.columns = ['sn', 'fault_time', 'prediction']\n",
    "    outcome_df = pd.merge(label_df, prediction_df ,how = 'left', on = ['sn', 'fault_time'])\n",
    "    weights = [5 / 11, 4 / 11, 1 / 11, 1 / 11]\n",
    "    macro_F1 = 0.\n",
    "    for i in range(len(weights)):\n",
    "        TP = len(outcome_df[(outcome_df['label'] == i) & (outcome_df['prediction'] == i)])\n",
    "        FP = len(outcome_df[(outcome_df['label'] != i) & (outcome_df['prediction'] == i)])\n",
    "        FN = len(outcome_df[(outcome_df['label'] == i) & (outcome_df['prediction'] != i)])\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        F1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        macro_F1 += weights[i] * F1\n",
    "        logger.info('Label {}:   Precision {: .2f}, Recall {: .2f}, F1 {: .2f}'.format(i, precision, recall, F1))\n",
    "    logger.info('macro_f1: {}\\n'.format(macro_F1))\n",
    "\n",
    "    return macro_F1\n",
    "\n",
    "# 模型训练函数\n",
    "def xgbTrain(feature_df: pd.DataFrame) -> xgb.XGBModel:\n",
    "    '''\n",
    "    feature_df: 要求最后3列为: label, sn, fault_time, 其余列均为特征\n",
    "    '''\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    logger = get_logger('./user_data/logs/{}_info.log'.format(date.today()), 'info')\n",
    "    logger_error = get_logger('./user_data/logs/{}_error.log'.format(date.today()), 'error')\n",
    "\n",
    "    feature_name_list = list(feature_df.columns)[0:-3]\n",
    "    feature_df = feature_df[(feature_df['msg_1_cnt'] != 0) \n",
    "                                    | (feature_df['msg_2_cnt'] != 0) \n",
    "                                    | (feature_df['msg_3_cnt'] != 0) \n",
    "                                    | (feature_df['msg_4_cnt'] != 0)]\n",
    "    feature = np.array(feature_df[feature_name_list])\n",
    "    label = np.array(feature_df['label'])\n",
    "    label_df = feature_df[['sn', 'fault_time', 'label']]\n",
    "    prediction_df = feature_df[['sn', 'fault_time']]\n",
    "\n",
    "    train_data = xgb.DMatrix(feature, label=label)\n",
    "    train_feature = xgb.DMatrix(feature)\n",
    "    logger.info('开始训练xgb模型')\n",
    "    xgb_model = xgb.train(xgb_params, train_data, num_boost_round=500)\n",
    "    logger.info('训练xgb模型结束')\n",
    "    # 训练集指标评估\n",
    "    prediction = xgb_model.predict(train_feature)\n",
    "    prediction_df['label'] = prediction\n",
    "    logger.info('训练集评估效果: ')\n",
    "    macro_f1(label_df, prediction_df)\n",
    "\n",
    "    return xgb_model\n",
    "\n",
    "# # xgb模型预测函数\n",
    "# def xgbPredict(model: xgb.XGBModel, feature_df: pd.DataFrame, label_df = None) -> pd.DataFrame:\n",
    "#     '''\n",
    "#         feature_df: 要求最后3列为: label, sn, fault_time, 其余列均为特征\n",
    "#     '''\n",
    "#     warnings.filterwarnings(\"ignore\")\n",
    "#     logger = get_logger('./user_data/logs/{}_info.log'.format(date.today()), 'info')\n",
    "#     logger_error = get_logger('./user_data/logs/{}_error.log'.format(date.today()), 'error')\n",
    "\n",
    "#     if label_df is None:\n",
    "#         logger.info('开始xgb模型预测')\n",
    "#         feature_name_list = list(feature_df.columns)[0:-3]\n",
    "#         na_msg_feature_df = feature_df[(feature_df['msg_1_cnt'] == 0) \n",
    "#                                     & (feature_df['msg_2_cnt'] == 0) \n",
    "#                                     & (feature_df['msg_3_cnt'] == 0) \n",
    "#                                     & (feature_df['msg_4_cnt'] == 0)]\n",
    "#         na_prediction_df = na_msg_feature_df[['sn', 'fault_time']]\n",
    "#         na_prediction_df['label'] = 2\n",
    "        \n",
    "#         notna_msg_feature_df = feature_df[(feature_df['msg_1_cnt'] != 0) \n",
    "#                                     | (feature_df['msg_2_cnt'] != 0) \n",
    "#                                     | (feature_df['msg_3_cnt'] != 0) \n",
    "#                                     | (feature_df['msg_4_cnt'] != 0)]\n",
    "#         notna_msg_feature = np.array(notna_msg_feature_df[feature_name_list])\n",
    "#         notna_prediction_df = notna_msg_feature_df[['sn', 'fault_time']]\n",
    "#         test_feature = xgb.DMatrix(notna_msg_feature)\n",
    "#         prediction = model.predict(test_feature)\n",
    "#         notna_prediction_df['label'] = prediction\n",
    "        \n",
    "#         prediction_df = pd.concat([notna_prediction_df, na_prediction_df])\n",
    "#         prediction_df['label'] = prediction_df['label'].apply(lambda x: int(x))\n",
    "#         logger.info('xgb模型预测结束')\n",
    "#     else:\n",
    "#         logger.info('开始xgb模型预测')\n",
    "#         feature_name_list = list(feature_df.columns)[0:-3]\n",
    "#         na_msg_feature_df = feature_df[(feature_df['msg_1_cnt'] == 0) \n",
    "#                                     & (feature_df['msg_2_cnt'] == 0) \n",
    "#                                     & (feature_df['msg_3_cnt'] == 0) \n",
    "#                                     & (feature_df['msg_4_cnt'] == 0)]\n",
    "#         na_prediction_df = na_msg_feature_df[['sn', 'fault_time']]\n",
    "#         na_prediction_df['label'] = 2\n",
    "        \n",
    "#         notna_msg_feature_df = feature_df[(feature_df['msg_1_cnt'] != 0) \n",
    "#                                     | (feature_df['msg_2_cnt'] != 0) \n",
    "#                                     | (feature_df['msg_3_cnt'] != 0) \n",
    "#                                     | (feature_df['msg_4_cnt'] != 0)]\n",
    "#         notna_msg_feature = np.array(notna_msg_feature_df[feature_name_list])\n",
    "#         notna_prediction_df = notna_msg_feature_df[['sn', 'fault_time']]\n",
    "#         test_feature = xgb.DMatrix(notna_msg_feature)\n",
    "#         prediction = model.predict(test_feature)\n",
    "#         notna_prediction_df['label'] = prediction\n",
    "        \n",
    "#         prediction_df = pd.concat([notna_prediction_df, na_prediction_df])\n",
    "#         prediction_df['label'] = prediction_df['label'].apply(lambda x: int(x))\n",
    "#         logger.info('xgb模型预测结束')\n",
    "#         # 测试集指标评估\n",
    "#         logger.info('测试集评估效果: ')\n",
    "#         macro_f1(label_df, prediction_df)\n",
    "\n",
    "#     return prediction_df\n",
    "\n",
    "\n",
    "# # xgb模型随机训练并投票预测\n",
    "# def xgbRandomTrainPredict(train_feature_df: pd.DataFrame, test_feature_df: pd.DataFrame, label_df = None) -> pd.DataFrame:\n",
    "#     ## 每个子模型样本均衡，利用投票规则生成最终预测\n",
    "#     random.seed(0)\n",
    "#     N = 100 # number of the models\n",
    "#     num_sample = 500 # number of samples for each label\n",
    "\n",
    "#     na_msg_test_df = test_feature_df[(test_feature_df['msg_1_cnt'] == 0) \n",
    "#                             & (test_feature_df['msg_2_cnt'] == 0) \n",
    "#                             & (test_feature_df['msg_3_cnt'] == 0) \n",
    "#                             & (test_feature_df['msg_4_cnt'] == 0)]\n",
    "#     na_prediction_df = na_msg_test_df[['sn', 'fault_time']]\n",
    "#     na_prediction_df['label'] = 2\n",
    "\n",
    "#     train_feature_df = train_feature_df[(train_feature_df['msg_1_cnt'] != 0) \n",
    "#                                 | (train_feature_df['msg_2_cnt'] != 0) \n",
    "#                                 | (train_feature_df['msg_3_cnt'] != 0) \n",
    "#                                 | (train_feature_df['msg_4_cnt'] != 0)]\n",
    "#     test_feature_df = test_feature_df[(test_feature_df['msg_1_cnt'] != 0) \n",
    "#                             | (test_feature_df['msg_2_cnt'] != 0) \n",
    "#                             | (test_feature_df['msg_3_cnt'] != 0) \n",
    "#                             | (test_feature_df['msg_4_cnt'] != 0)]\n",
    "#     _label0_index_list = list(train_feature_df[train_feature_df['label'] == 0].index)\n",
    "#     _label1_index_list = list(train_feature_df[train_feature_df['label'] == 1].index)\n",
    "#     _label2_index_list = list(train_feature_df[train_feature_df['label'] == 2].index)\n",
    "#     _label3_index_list = list(train_feature_df[train_feature_df['label'] == 3].index)\n",
    "#     feature_name_list = list(train_feature_df.columns)[0:-3]\n",
    "#     test_feature = np.array(test_feature_df[feature_name_list])\n",
    "#     test_feature = xgb.DMatrix(test_feature)\n",
    "#     prediction_df = test_feature_df[['sn', 'fault_time']]\n",
    "\n",
    "#     for iter in np.arange(N):\n",
    "#         idx_0 = random.sample(_label0_index_list, num_sample)\n",
    "#         idx_1 = random.sample(_label1_index_list, num_sample)\n",
    "#         idx_2 = random.sample(_label2_index_list, num_sample)\n",
    "#         idx_3 = random.sample(_label3_index_list, num_sample)\n",
    "#         idx = np.hstack((idx_0, idx_1, idx_2, idx_3))\n",
    "#         random.shuffle(idx)\n",
    "#         sub_train_feature_df = train_feature_df.loc[idx, :]\n",
    "#         sub_train_feature = np.array(sub_train_feature_df[feature_name_list])\n",
    "#         sub_train_label = np.array(sub_train_feature_df['label'])\n",
    "#         sub_train_data = xgb.DMatrix(sub_train_feature,label = sub_train_label)\n",
    "\n",
    "#         logger.info('开始第{}轮训练和预测'.format(iter))\n",
    "#         sub_xgb_model = xgb.train(xgb_params, sub_train_data, num_boost_round=500)\n",
    "#         sub_test_pred = sub_xgb_model.predict(test_feature)\n",
    "#         if iter == 0:\n",
    "#             val_pred = sub_test_pred\n",
    "#         else:\n",
    "#             val_pred = np.vstack((val_pred, sub_test_pred))\n",
    "#         logger.info('第{}轮训练和预测结束'.format(iter))\n",
    "\n",
    "#     # 训练集指标评估\n",
    "#     final_pred = [np.argmax(np.bincount(val_pred[:, i].astype(int))) for i in np.arange(val_pred.shape[1])]\n",
    "#     final_pred = np.array(final_pred).astype(int)\n",
    "#     prediction_df['label'] = final_pred\n",
    "#     prediction_df = pd.concat([prediction_df, na_prediction_df])\n",
    "#     if label_df is not None:\n",
    "#         logger.info('训练集评估效果: ')\n",
    "#         macro_f1(label_df, prediction_df)\n",
    "    \n",
    "#     return prediction_df\n",
    "\n",
    "\n",
    "# xgb模型预测函数\n",
    "def xgbPredict(model: xgb.XGBModel, feature_df: pd.DataFrame, label_df = None) -> pd.DataFrame:\n",
    "    '''\n",
    "        feature_df: 要求最后3列为: label, sn, fault_time, 其余列均为特征\n",
    "    '''\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    logger = get_logger('./user_data/logs/{}_info.log'.format(date.today()), 'info')\n",
    "    logger_error = get_logger('./user_data/logs/{}_error.log'.format(date.today()), 'error')\n",
    "\n",
    "    if label_df is None:\n",
    "        feature_name_list = list(feature_df.columns)[0:-3]\n",
    "        feature = np.array(feature_df[feature_name_list])\n",
    "        prediction_df = feature_df[['sn', 'fault_time']]\n",
    "\n",
    "        test_feature = xgb.DMatrix(feature)\n",
    "        logger.info('开始xgb模型预测')\n",
    "        prediction = model.predict(test_feature)\n",
    "        logger.info('xgb模型预测结束')\n",
    "        prediction_df['label'] = prediction\n",
    "        prediction_df['label'] = prediction_df['label'].apply(lambda x: int(x))\n",
    "\n",
    "    else:\n",
    "        feature_name_list = list(feature_df.columns)[0:-3]\n",
    "        feature = np.array(feature_df[feature_name_list])\n",
    "        prediction_df = feature_df[['sn', 'fault_time']]\n",
    "\n",
    "        test_feature = xgb.DMatrix(feature)\n",
    "        logger.info('开始xgb模型预测')\n",
    "        prediction = model.predict(test_feature)\n",
    "        logger.info('xgb模型预测结束')\n",
    "        # 测试集指标评估\n",
    "        prediction_df['label'] = prediction\n",
    "        prediction_df['label'] = prediction_df['label'].apply(lambda x: int(x))\n",
    "        logger.info('测试集评估效果: ')\n",
    "        macro_f1(label_df, prediction_df)\n",
    "\n",
    "    return prediction_df\n",
    "\n",
    "\n",
    "# xgb模型随机训练并投票预测\n",
    "def xgbRandomTrainPredict(train_feature_df: pd.DataFrame, test_feature_df: pd.DataFrame, label_df = None) -> pd.DataFrame:\n",
    "    ## 每个子模型样本均衡，利用投票规则生成最终预测\n",
    "    random.seed(0)\n",
    "    N = 100 # number of the models\n",
    "    num_sample = 500 # number of samples for each label\n",
    "\n",
    "    _label0_index_list = list(train_feature_df[train_feature_df['label'] == 0].index)\n",
    "    _label1_index_list = list(train_feature_df[train_feature_df['label'] == 1].index)\n",
    "    _label2_index_list = list(train_feature_df[train_feature_df['label'] == 2].index)\n",
    "    _label3_index_list = list(train_feature_df[train_feature_df['label'] == 3].index)\n",
    "    feature_name_list = list(train_feature_df.columns)[0:-3]\n",
    "    test_feature = np.array(test_feature_df[feature_name_list])\n",
    "    test_feature = xgb.DMatrix(test_feature)\n",
    "    prediction_df = test_feature_df[['sn', 'fault_time']]\n",
    "\n",
    "    for iter in np.arange(N):\n",
    "        idx_0 = random.sample(_label0_index_list, num_sample)\n",
    "        idx_1 = random.sample(_label1_index_list, num_sample)\n",
    "        idx_2 = random.sample(_label2_index_list, num_sample)\n",
    "        idx_3 = random.sample(_label3_index_list, num_sample)\n",
    "        idx = np.hstack((idx_0, idx_1, idx_2, idx_3))\n",
    "        random.shuffle(idx)\n",
    "        sub_train_feature_df = train_feature_df.loc[idx, :]\n",
    "        sub_train_feature = np.array(sub_train_feature_df[feature_name_list])\n",
    "        sub_train_label = np.array(sub_train_feature_df['label'])\n",
    "        sub_train_data = xgb.DMatrix(sub_train_feature,label = sub_train_label)\n",
    "\n",
    "        logger.info('开始第{}轮训练和预测'.format(iter))\n",
    "        sub_xgb_model = xgb.train(xgb_params, sub_train_data, num_boost_round=500)\n",
    "        sub_test_pred = sub_xgb_model.predict(test_feature)\n",
    "        if iter == 0:\n",
    "            val_pred = sub_test_pred\n",
    "        else:\n",
    "            val_pred = np.vstack((val_pred, sub_test_pred))\n",
    "        logger.info('第{}轮训练和预测结束'.format(iter))\n",
    "\n",
    "    # 训练集指标评估\n",
    "    final_pred = [np.argmax(np.bincount(val_pred[:, i].astype(int))) for i in np.arange(val_pred.shape[1])]\n",
    "    final_pred = np.array(final_pred).astype(int)\n",
    "    prediction_df['label'] = final_pred\n",
    "    if label_df is not None:\n",
    "        logger.info('训练集评估效果: ')\n",
    "        macro_f1(label_df, prediction_df)\n",
    "    \n",
    "    return prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "56371e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(sys.path[0]))\n",
    "# 忽略warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = get_logger('./user_data/logs/{}_info.log'.format(date.today()), 'info')\n",
    "logger_error = get_logger('./user_data/logs/{}_error.log'.format(date.today()), 'error')\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "# 读取sel日志数据\n",
    "sel_log_df = pd.read_csv('./data/preliminary_train/preliminary_sel_log_dataset.csv').drop_duplicates()\n",
    "# 读取训练标签数据：有重复数据！\n",
    "train_label1 = pd.read_csv('./data/preliminary_train/preliminary_train_label_dataset.csv')\n",
    "train_label2 = pd.read_csv('./data/preliminary_train/preliminary_train_label_dataset_s.csv')\n",
    "train_label_df = pd.concat([train_label1,train_label2],axis=0).drop_duplicates()\n",
    "# 读取词列表\n",
    "v1_word_list = list(pd.read_csv('./user_data/words/word_frequency_df.txt',sep='\\t')['word'])\n",
    "v1p1_word_list = list(pd.read_csv('./user_data/words/tags_incomplete.txt',sep='\\t',names=['word'])['word'])\n",
    "word_list = list(set(v1_word_list+v1p1_word_list))\n",
    "important_word_list = list(pd.read_csv('./user_data/words/important_word_df.csv')['word'])\n",
    "word_list = important_word_list\n",
    "\n",
    "# 获取特征\n",
    "# 计算特征\n",
    "# train_feature_df = caculateFeature(sel_log_df, train_label_df, word_list)\n",
    "# train_feature_df.to_csv('./user_data/feature_data/nearesttime_train_feature_300_df.csv', index=None)\n",
    "# train_feature_df = pd.read_csv('./user_data/feature_data/faulttime_train_feature_all_df.csv')\n",
    "# train_feature_df = pd.read_csv('./user_data/feature_data/faulttime_train_feature_300_df.csv')\n",
    "train_feature_df = pd.read_csv('./user_data/feature_data/nearesttime_train_feature_300_df.csv')\n",
    "random.seed(0)\n",
    "val_mask = [random.random() < 0.3 for _ in range(len(train_feature_df))]\n",
    "train_mask = [not xx for xx in val_mask]\n",
    "temp_feature_df = train_feature_df[train_mask]\n",
    "val_feature_df = train_feature_df[val_mask]\n",
    "# val_feature_df = val_feature_df[(val_feature_df['msg_1_cnt'] == 0) & (val_feature_df['msg_2_cnt'] == 0) & (val_feature_df['msg_3_cnt'] == 0) & (val_feature_df['msg_4_cnt'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3c253f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-15 18:45:04,203 5912 3442391308.py[line:428] - INFO: 开始训练xgb模型\n",
      "[18:45:04] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:45:04] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "2022-05-15 18:45:19,433 5912 3442391308.py[line:430] - INFO: 训练xgb模型结束\n",
      "2022-05-15 18:45:19,468 5912 3442391308.py[line:434] - INFO: 训练集评估效果: \n",
      "2022-05-15 18:45:19,482 5912 3442391308.py[line:402] - INFO: Label 0:   Precision  1.00, Recall  1.00, F1  1.00\n",
      "2022-05-15 18:45:19,484 5912 3442391308.py[line:402] - INFO: Label 1:   Precision  1.00, Recall  1.00, F1  1.00\n",
      "2022-05-15 18:45:19,486 5912 3442391308.py[line:402] - INFO: Label 2:   Precision  1.00, Recall  1.00, F1  1.00\n",
      "2022-05-15 18:45:19,489 5912 3442391308.py[line:402] - INFO: Label 3:   Precision  1.00, Recall  1.00, F1  1.00\n",
      "2022-05-15 18:45:19,489 5912 3442391308.py[line:403] - INFO: macro_f1: 1.0\n",
      "\n",
      "2022-05-15 18:45:19,493 5912 3442391308.py[line:472] - INFO: 开始xgb模型预测\n",
      "2022-05-15 18:45:19,593 5912 3442391308.py[line:493] - INFO: xgb模型预测结束\n",
      "2022-05-15 18:45:19,593 5912 3442391308.py[line:495] - INFO: 测试集评估效果: \n",
      "2022-05-15 18:45:19,612 5912 3442391308.py[line:402] - INFO: Label 0:   Precision  1.00, Recall  1.00, F1  1.00\n",
      "2022-05-15 18:45:19,615 5912 3442391308.py[line:402] - INFO: Label 1:   Precision  1.00, Recall  0.96, F1  0.98\n",
      "2022-05-15 18:45:19,617 5912 3442391308.py[line:402] - INFO: Label 2:   Precision  0.97, Recall  1.00, F1  0.99\n",
      "2022-05-15 18:45:19,619 5912 3442391308.py[line:402] - INFO: Label 3:   Precision  1.00, Recall  0.96, F1  0.98\n",
      "2022-05-15 18:45:19,620 5912 3442391308.py[line:403] - INFO: macro_f1: 0.9884877403075933\n",
      "\n",
      "2022-05-15 18:45:19,626 5912 3442391308.py[line:472] - INFO: 开始xgb模型预测\n",
      "2022-05-15 18:45:19,686 5912 3442391308.py[line:493] - INFO: xgb模型预测结束\n",
      "2022-05-15 18:45:19,686 5912 3442391308.py[line:495] - INFO: 测试集评估效果: \n",
      "2022-05-15 18:45:19,695 5912 3442391308.py[line:402] - INFO: Label 0:   Precision  0.58, Recall  0.35, F1  0.44\n",
      "2022-05-15 18:45:19,697 5912 3442391308.py[line:402] - INFO: Label 1:   Precision  0.72, Recall  0.75, F1  0.73\n",
      "2022-05-15 18:45:19,700 5912 3442391308.py[line:402] - INFO: Label 2:   Precision  0.92, Recall  0.97, F1  0.94\n",
      "2022-05-15 18:45:19,702 5912 3442391308.py[line:402] - INFO: Label 3:   Precision  0.91, Recall  0.89, F1  0.90\n",
      "2022-05-15 18:45:19,702 5912 3442391308.py[line:403] - INFO: macro_f1: 0.6351683736404633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# xgb训练预测\n",
    "# 报错时间 - 直接\n",
    "0.6648\n",
    "# 报错时间 - 投票\n",
    "0.6655\n",
    "# 报错时间 - 取出na后直接\n",
    "0.6359\n",
    "# 报错时间 - 取出na后投票\n",
    "0.6219\n",
    "# 报错时间 - 训练去na,测试保留\n",
    "0.6359\n",
    "\n",
    "# 最近邻时间 - 直接\n",
    "0.6559\n",
    "# 最近邻时间 - 投票\n",
    "0.6745\n",
    "# 最近邻时间 - 取出na后直接\n",
    "0.6352\n",
    "# 最近邻时间 - 取出na后投票\n",
    "0.6295\n",
    "# 最近邻时间 - 训练去na,测试保留\n",
    "0.6352\n",
    "\n",
    "model = xgbTrain(temp_feature_df)\n",
    "temp_prediction_df = xgbPredict(model, temp_feature_df, temp_feature_df[['sn', 'fault_time', 'label']])\n",
    "val_prediction_df = xgbPredict(model, val_feature_df, val_feature_df[['sn', 'fault_time', 'label']])\n",
    "# predictions = xgbRandomTrainPredict(temp_feature_df, val_feature_df, val_feature_df[['sn', 'fault_time', 'label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4194c144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sn</th>\n",
       "      <th>fault_time</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SERVER_10001</td>\n",
       "      <td>2020-05-01 10:04:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SERVER_10003</td>\n",
       "      <td>2020-03-28 09:48:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SERVER_10008</td>\n",
       "      <td>2020-02-25 16:12:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SERVER_10009</td>\n",
       "      <td>2020-05-08 16:37:00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SERVER_10012</td>\n",
       "      <td>2020-07-13 03:32:00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16598</th>\n",
       "      <td>SERVER_999</td>\n",
       "      <td>2020-10-20 19:44:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16599</th>\n",
       "      <td>SERVER_9991</td>\n",
       "      <td>2020-08-04 22:49:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16600</th>\n",
       "      <td>SERVER_9991</td>\n",
       "      <td>2020-10-07 18:42:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16601</th>\n",
       "      <td>SERVER_9993</td>\n",
       "      <td>2020-05-14 23:50:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16603</th>\n",
       "      <td>SERVER_9999</td>\n",
       "      <td>2020-10-13 02:57:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11617 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 sn           fault_time  prediction\n",
       "0      SERVER_10001  2020-05-01 10:04:00           1\n",
       "1      SERVER_10003  2020-03-28 09:48:00           2\n",
       "2      SERVER_10008  2020-02-25 16:12:00           1\n",
       "4      SERVER_10009  2020-05-08 16:37:00           3\n",
       "5      SERVER_10012  2020-07-13 03:32:00           3\n",
       "...             ...                  ...         ...\n",
       "16598    SERVER_999  2020-10-20 19:44:00           2\n",
       "16599   SERVER_9991  2020-08-04 22:49:00           2\n",
       "16600   SERVER_9991  2020-10-07 18:42:00           2\n",
       "16601   SERVER_9993  2020-05-14 23:50:00           2\n",
       "16603   SERVER_9999  2020-10-13 02:57:00           2\n",
       "\n",
       "[11617 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "815c9236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prediction\n",
       "1      5\n",
       "2    138\n",
       "3     16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_prediction_df.groupby('prediction').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65133f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sn</th>\n",
       "      <th>fault_time</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>SERVER_10110</td>\n",
       "      <td>2020-05-17 18:22:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>SERVER_10111</td>\n",
       "      <td>2020-05-06 15:51:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>SERVER_10167</td>\n",
       "      <td>2020-03-13 18:25:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>SERVER_10186</td>\n",
       "      <td>2020-05-09 14:48:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>SERVER_10251</td>\n",
       "      <td>2020-11-15 23:59:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15011</th>\n",
       "      <td>SERVER_7696</td>\n",
       "      <td>2020-06-27 11:16:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15221</th>\n",
       "      <td>SERVER_8002</td>\n",
       "      <td>2020-03-25 14:10:00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15266</th>\n",
       "      <td>SERVER_8074</td>\n",
       "      <td>2020-01-10 13:23:00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15887</th>\n",
       "      <td>SERVER_9041</td>\n",
       "      <td>2020-08-26 04:17:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16473</th>\n",
       "      <td>SERVER_9855</td>\n",
       "      <td>2020-05-24 02:13:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 sn           fault_time  prediction\n",
       "71     SERVER_10110  2020-05-17 18:22:00           2\n",
       "75     SERVER_10111  2020-05-06 15:51:00           2\n",
       "128    SERVER_10167  2020-03-13 18:25:00           2\n",
       "149    SERVER_10186  2020-05-09 14:48:00           2\n",
       "232    SERVER_10251  2020-11-15 23:59:00           2\n",
       "...             ...                  ...         ...\n",
       "15011   SERVER_7696  2020-06-27 11:16:00           2\n",
       "15221   SERVER_8002  2020-03-25 14:10:00           3\n",
       "15266   SERVER_8074  2020-01-10 13:23:00           3\n",
       "15887   SERVER_9041  2020-08-26 04:17:00           2\n",
       "16473   SERVER_9855  2020-05-24 02:13:00           2\n",
       "\n",
       "[159 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbbdc431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sn</th>\n",
       "      <th>fault_time</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>SERVER_10110</td>\n",
       "      <td>2020-05-17 18:22:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>SERVER_10111</td>\n",
       "      <td>2020-05-06 15:51:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>SERVER_10167</td>\n",
       "      <td>2020-03-13 18:25:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>SERVER_10186</td>\n",
       "      <td>2020-05-09 14:48:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>SERVER_10251</td>\n",
       "      <td>2020-11-15 23:59:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15011</th>\n",
       "      <td>SERVER_7696</td>\n",
       "      <td>2020-06-27 11:16:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15221</th>\n",
       "      <td>SERVER_8002</td>\n",
       "      <td>2020-03-25 14:10:00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15266</th>\n",
       "      <td>SERVER_8074</td>\n",
       "      <td>2020-01-10 13:23:00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15887</th>\n",
       "      <td>SERVER_9041</td>\n",
       "      <td>2020-08-26 04:17:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16473</th>\n",
       "      <td>SERVER_9855</td>\n",
       "      <td>2020-05-24 02:13:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 sn           fault_time  prediction\n",
       "71     SERVER_10110  2020-05-17 18:22:00           2\n",
       "75     SERVER_10111  2020-05-06 15:51:00           2\n",
       "128    SERVER_10167  2020-03-13 18:25:00           2\n",
       "149    SERVER_10186  2020-05-09 14:48:00           2\n",
       "232    SERVER_10251  2020-11-15 23:59:00           2\n",
       "...             ...                  ...         ...\n",
       "15011   SERVER_7696  2020-06-27 11:16:00           2\n",
       "15221   SERVER_8002  2020-03-25 14:10:00           3\n",
       "15266   SERVER_8074  2020-01-10 13:23:00           3\n",
       "15887   SERVER_9041  2020-08-26 04:17:00           2\n",
       "16473   SERVER_9855  2020-05-24 02:13:00           2\n",
       "\n",
       "[159 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_prediction_df.rename(columns = {'prediction': })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14ffbd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "faulttime_na_predict_df = pd.merge(val_prediction_df, val_feature_df, how = 'left', on = ['sn', 'fault_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b95f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "faulttime_na_predict_df = faulttime_na_predict_df[['sn', 'fault_time', 'label', 'prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "224b24a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sn</th>\n",
       "      <th>fault_time</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SERVER_10936</td>\n",
       "      <td>2020-05-22 00:49:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SERVER_11461</td>\n",
       "      <td>2020-04-09 02:20:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SERVER_12330</td>\n",
       "      <td>2020-01-07 18:25:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SERVER_12345</td>\n",
       "      <td>2020-03-05 22:46:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SERVER_12530</td>\n",
       "      <td>2020-10-24 01:24:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SERVER_12877</td>\n",
       "      <td>2020-01-23 11:45:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SERVER_13006</td>\n",
       "      <td>2020-06-08 22:07:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>SERVER_13920</td>\n",
       "      <td>2020-01-27 04:23:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>SERVER_13984</td>\n",
       "      <td>2020-06-16 10:57:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>SERVER_1516</td>\n",
       "      <td>2020-10-02 12:43:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>SERVER_15234</td>\n",
       "      <td>2020-11-22 11:37:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>SERVER_16331</td>\n",
       "      <td>2020-01-04 11:16:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>SERVER_16985</td>\n",
       "      <td>2020-05-02 16:54:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>SERVER_17006</td>\n",
       "      <td>2020-01-29 01:29:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>SERVER_17218</td>\n",
       "      <td>2020-08-14 21:41:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>SERVER_18031</td>\n",
       "      <td>2020-09-30 08:16:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>SERVER_18090</td>\n",
       "      <td>2020-09-24 16:34:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>SERVER_1814</td>\n",
       "      <td>2020-05-07 11:44:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>SERVER_18832</td>\n",
       "      <td>2020-01-23 21:08:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>SERVER_1888</td>\n",
       "      <td>2020-02-04 18:42:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>SERVER_19083</td>\n",
       "      <td>2020-08-05 02:08:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>SERVER_19094</td>\n",
       "      <td>2020-05-17 22:59:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>SERVER_19893</td>\n",
       "      <td>2020-10-05 21:05:00</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>SERVER_20394</td>\n",
       "      <td>2020-09-05 07:22:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>SERVER_20439</td>\n",
       "      <td>2020-10-14 04:12:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>SERVER_2116</td>\n",
       "      <td>2020-08-26 18:28:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>SERVER_2130</td>\n",
       "      <td>2020-08-25 20:51:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>SERVER_21406</td>\n",
       "      <td>2020-08-27 18:54:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>SERVER_21417</td>\n",
       "      <td>2019-12-31 02:51:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>SERVER_22786</td>\n",
       "      <td>2020-08-06 10:35:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>SERVER_22893</td>\n",
       "      <td>2020-09-02 22:32:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>SERVER_23459</td>\n",
       "      <td>2020-10-24 07:07:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>SERVER_23665</td>\n",
       "      <td>2020-10-29 09:56:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>SERVER_24158</td>\n",
       "      <td>2020-06-12 16:05:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>SERVER_3065</td>\n",
       "      <td>2020-03-11 07:51:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>SERVER_3278</td>\n",
       "      <td>2020-06-24 17:03:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>SERVER_3352</td>\n",
       "      <td>2020-08-20 17:41:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>SERVER_4299</td>\n",
       "      <td>2020-02-03 04:33:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>SERVER_460</td>\n",
       "      <td>2020-10-03 16:12:00</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>SERVER_4684</td>\n",
       "      <td>2020-08-22 10:09:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>SERVER_5899</td>\n",
       "      <td>2020-09-30 16:28:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>SERVER_6158</td>\n",
       "      <td>2020-11-04 15:04:00</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>SERVER_6601</td>\n",
       "      <td>2020-03-19 17:38:00</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>SERVER_7696</td>\n",
       "      <td>2020-06-27 11:16:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>SERVER_9041</td>\n",
       "      <td>2020-08-26 04:17:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               sn           fault_time  label  prediction\n",
       "6    SERVER_10936  2020-05-22 00:49:00      1           2\n",
       "10   SERVER_11461  2020-04-09 02:20:00      1           2\n",
       "12   SERVER_12330  2020-01-07 18:25:00      3           2\n",
       "13   SERVER_12345  2020-03-05 22:46:00      3           2\n",
       "14   SERVER_12530  2020-10-24 01:24:00      1           2\n",
       "15   SERVER_12877  2020-01-23 11:45:00      1           2\n",
       "19   SERVER_13006  2020-06-08 22:07:00      3           2\n",
       "30   SERVER_13920  2020-01-27 04:23:00      1           2\n",
       "32   SERVER_13984  2020-06-16 10:57:00      2           1\n",
       "43    SERVER_1516  2020-10-02 12:43:00      1           2\n",
       "45   SERVER_15234  2020-11-22 11:37:00      1           2\n",
       "46   SERVER_16331  2020-01-04 11:16:00      1           2\n",
       "49   SERVER_16985  2020-05-02 16:54:00      1           3\n",
       "51   SERVER_17006  2020-01-29 01:29:00      1           3\n",
       "52   SERVER_17218  2020-08-14 21:41:00      2           1\n",
       "54   SERVER_18031  2020-09-30 08:16:00      1           2\n",
       "55   SERVER_18090  2020-09-24 16:34:00      1           2\n",
       "57    SERVER_1814  2020-05-07 11:44:00      1           2\n",
       "60   SERVER_18832  2020-01-23 21:08:00      3           2\n",
       "62    SERVER_1888  2020-02-04 18:42:00      1           2\n",
       "67   SERVER_19083  2020-08-05 02:08:00      3           2\n",
       "68   SERVER_19094  2020-05-17 22:59:00      1           2\n",
       "71   SERVER_19893  2020-10-05 21:05:00      2           3\n",
       "73   SERVER_20394  2020-09-05 07:22:00      1           3\n",
       "74   SERVER_20439  2020-10-14 04:12:00      1           2\n",
       "79    SERVER_2116  2020-08-26 18:28:00      3           2\n",
       "82    SERVER_2130  2020-08-25 20:51:00      3           2\n",
       "83   SERVER_21406  2020-08-27 18:54:00      3           2\n",
       "85   SERVER_21417  2019-12-31 02:51:00      3           2\n",
       "88   SERVER_22786  2020-08-06 10:35:00      1           2\n",
       "89   SERVER_22893  2020-09-02 22:32:00      2           1\n",
       "94   SERVER_23459  2020-10-24 07:07:00      3           2\n",
       "96   SERVER_23665  2020-10-29 09:56:00      3           2\n",
       "100  SERVER_24158  2020-06-12 16:05:00      3           2\n",
       "129   SERVER_3065  2020-03-11 07:51:00      3           2\n",
       "130   SERVER_3278  2020-06-24 17:03:00      3           2\n",
       "133   SERVER_3352  2020-08-20 17:41:00      1           2\n",
       "137   SERVER_4299  2020-02-03 04:33:00      1           2\n",
       "140    SERVER_460  2020-10-03 16:12:00      2           3\n",
       "141   SERVER_4684  2020-08-22 10:09:00      2           1\n",
       "144   SERVER_5899  2020-09-30 16:28:00      1           2\n",
       "145   SERVER_6158  2020-11-04 15:04:00      2           3\n",
       "147   SERVER_6601  2020-03-19 17:38:00      2           3\n",
       "154   SERVER_7696  2020-06-27 11:16:00      1           2\n",
       "157   SERVER_9041  2020-08-26 04:17:00      1           2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faulttime_na_predict_df[faulttime_na_predict_df['label'] != faulttime_na_predict_df['prediction']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Log_diagosis_python",
   "language": "python",
   "name": "log_diagosis_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
