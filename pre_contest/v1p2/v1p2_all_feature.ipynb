{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c0ace7",
   "metadata": {},
   "source": [
    "# 背景：将v1_baseline词频向量、v1p1词频向量、word2cev词向量合并训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d7456b",
   "metadata": {},
   "source": [
    "## 导包、设置根目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2893e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\workfile\\python\\Log-diagnosis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinca\\anaconda3\\envs\\Log_diagnosis_python\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\jinca\\anaconda3\\envs\\Log_diagnosis_python\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import datetime\n",
    "\n",
    "# 更改工作目录为当前项目根目录\n",
    "import sys\n",
    "import os\n",
    "os.chdir(os.path.dirname(os.path.dirname(sys.path[0])))\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c455b9",
   "metadata": {},
   "source": [
    "## 读取日志和标签数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a858c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    1460\n",
       "1    3016\n",
       "2    7731\n",
       "3    2214\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df = pd.read_csv('./pre_contest/v1p2/word_label_df.txt',sep='\\t',index_col=0)\n",
    "log_list=list(word_df['log'])\n",
    "label_list=list(word_df['label'])\n",
    "word_df.groupby('label').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263e661",
   "metadata": {},
   "source": [
    "## 训练word2vec向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dde4d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取额外的语料库并对日志按sn+day分组去重合并\n",
    "additional_sel_log_df=pd.read_csv('./pre_contest/dataset/additional_sel_log_dataset.csv').drop_duplicates()\n",
    "additional_sel_log_df['day']=additional_sel_log_df['time'].apply(lambda x:x[0:10])\n",
    "# 去重合并同一天的日志\n",
    "sn_list=[]\n",
    "day_list=[]\n",
    "sel_log_str_list=[]\n",
    "for log_df in additional_sel_log_df.groupby(['sn','day']):\n",
    "    log_str=''\n",
    "    sn_list.append(log_df[0][0])\n",
    "    day_list.append(log_df[0][1])\n",
    "    for info in log_df[1]['msg'].drop_duplicates():\n",
    "        if info==info:\n",
    "            log_str=log_str+info.lower()+'.'\n",
    "    sel_log_str_list.append(log_str)\n",
    "additional_sel_word_df=pd.DataFrame({'sn':sn_list,'day':day_list,'log':sel_log_str_list})\n",
    "# additional_sel_word_df.to_csv('./pre_contest/v1p2/additional_sel_word_df.csv',sep=',',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ba2ea1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86962"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_log_list=list(additional_sel_word_df['log'])\n",
    "all_log_list=log_list+sel_log_list\n",
    "len(all_log_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "66e21b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对标签日志进行分词\n",
    "raw_word_list = [word_tokenize(ith) for ith in log_list]\n",
    "word_list=[]\n",
    "for i in range(len(raw_word_list)):\n",
    "    xth=[]\n",
    "    for word in raw_word_list[i]:\n",
    "        word_drop=re.sub(r'[^\\w]','',str(word)).lower()\n",
    "        if word_drop:\n",
    "            xth.append(word_drop)\n",
    "    word_list.append(xth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "22efe94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对所有日志进行分词\n",
    "raw_word_list = [word_tokenize(ith) for ith in all_log_list]\n",
    "all_word_list=[]\n",
    "for i in range(len(raw_word_list)):\n",
    "    xth=[]\n",
    "    for word in raw_word_list[i]:\n",
    "        word_drop=re.sub(r'[^\\w]','',str(word)).lower()\n",
    "        if word_drop:\n",
    "            xth.append(word_drop)\n",
    "    all_word_list.append(xth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6fea21e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86962"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "34f4dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用所有日志训练Word2Vec模型\n",
    "word2vec_model = Word2Vec(word_list,vector_size=500, alpha=0.03, window=5, min_count=1,max_vocab_size=None, sample=1e-3, seed=0, workers=12, min_alpha=0.0001,sg=1, hs=0, negative=5, cbow_mean=1, hashfxn=hash, epochs=50, null_word=0,trim_rule=None, sorted_vocab=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c0cbb159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练词向量\n",
    "word2vec_vector_list=[]\n",
    "for word in word_list:\n",
    "    vector_sum=[]\n",
    "    for item in word:\n",
    "        vector=word2vec_model.wv[item].reshape(1,-1)[0]\n",
    "        if len(vector_sum)>0:\n",
    "            vector_sum=list(np.array(vector_sum)+np.array(vector))\n",
    "        else:\n",
    "            vector_sum=list(np.array(vector))\n",
    "    word2vec_vector_list.append([xth/len(word) for xth in vector_sum])\n",
    "feature=np.array(word2vec_vector_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f28503",
   "metadata": {},
   "source": [
    "## 读取v1_baseline用的词和v1p1的新词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7a2fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_word_list=list(pd.read_csv('pre_contest/v1p2/word_frequency_df.txt',sep='\\t')['word'])\n",
    "v1p1_word_list=list(pd.read_csv('pre_contest/v1p2/tags_incomplete.txt',sep='\\t',names=['word'])['word'])\n",
    "v1p2_word_list=list(set(v1_word_list+v1p1_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23c2b235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2087"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v1p2_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac50a85",
   "metadata": {},
   "source": [
    "## 训练词频向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6786435d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2022-03-20 20:48:54.711841\n",
      "100 2022-03-20 20:48:56.281347\n",
      "200 2022-03-20 20:48:57.848320\n",
      "300 2022-03-20 20:48:59.431320\n",
      "400 2022-03-20 20:49:01.051340\n",
      "500 2022-03-20 20:49:02.644856\n",
      "600 2022-03-20 20:49:04.282951\n",
      "700 2022-03-20 20:49:05.873979\n",
      "800 2022-03-20 20:49:07.443504\n",
      "900 2022-03-20 20:49:09.045040\n",
      "1000 2022-03-20 20:49:10.644079\n",
      "1100 2022-03-20 20:49:12.213601\n",
      "1200 2022-03-20 20:49:13.734571\n",
      "1300 2022-03-20 20:49:15.311529\n",
      "1400 2022-03-20 20:49:16.870801\n",
      "1500 2022-03-20 20:49:18.404746\n",
      "1600 2022-03-20 20:49:19.933220\n",
      "1700 2022-03-20 20:49:21.483719\n",
      "1800 2022-03-20 20:49:23.041682\n",
      "1900 2022-03-20 20:49:24.592136\n",
      "2000 2022-03-20 20:49:26.184170\n"
     ]
    }
   ],
   "source": [
    "frequency_vector_list = []\n",
    "tag=0\n",
    "for word in v1p2_word_list:\n",
    "    if tag%100==0:\n",
    "        print(tag,datetime.datetime.now())\n",
    "    pattern=re.compile(word)\n",
    "    frequency_vector = [len(re.findall(pattern,log))  for log in log_list]\n",
    "    frequency_vector_list.append(frequency_vector)\n",
    "    tag+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7f606646",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_vector_df=pd.DataFrame(frequency_vector_list)\n",
    "frequency_vector_df=frequency_vector_df.T\n",
    "frequency_vector_df.columns=v1p2_word_list\n",
    "frequency_vector_df['label']=label_list\n",
    "frequency_vector_df['sn']=word_df['sn']\n",
    "frequency_vector_df['day']=word_df['day']\n",
    "feature=np.array(frequency_vector_df[v1p2_word_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8525fc7b",
   "metadata": {},
   "source": [
    "## 合并词频向量与word2vec词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "30467cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature=np.hstack((np.array(frequency_vector_df[v1p2_word_list]),np.array(word2vec_vector_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cacd2f",
   "metadata": {},
   "source": [
    "## 训练xgb模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0b292500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb模型参数\n",
    "xgb_params = {\n",
    "    'booster':'gbtree',\n",
    "    'objective':'multi:softmax',   # 多分类问题\n",
    "    'num_class':4,  # 类别数，与multi softmax并用\n",
    "    'gamma':0.1,    # 用于控制是否后剪枝的参数，越大越保守，一般0.1 0.2的样子\n",
    "    'max_depth':6,  # 构建树的深度，越大越容易过拟合\n",
    "    'lambda':2,  # 控制模型复杂度的权重值的L2 正则化项参数，参数越大，模型越不容易过拟合\n",
    "    'subsample':1, # 随机采样训练样本\n",
    "    'colsample_bytree':1,# 这个参数默认为1，是每个叶子里面h的和至少是多少\n",
    "    # 对于正负样本不均衡时的0-1分类而言，假设h在0.01附近，min_child_weight为1\n",
    "    #意味着叶子节点中最少需要包含100个样本。这个参数非常影响结果，\n",
    "    # 控制叶子节点中二阶导的和的最小值，该参数值越小，越容易过拟合\n",
    "    'silent':0,  # 设置成1 则没有运行信息输入，最好是设置成0\n",
    "    'eta':0.3,  # 如同学习率\n",
    "    'seed':1000,\n",
    "    'nthread':16,  #CPU线程数\n",
    "    #'eval_metric':'auc'\n",
    "}\n",
    "\n",
    "# 指标评估\n",
    "def macro_f1(label,prediction)  -> float:\n",
    "\n",
    "    \"\"\"\n",
    "    计算得分\n",
    "    :param target_df: [sn,fault_time,label]\n",
    "    :param submit_df: [sn,fault_time,label]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    weights =  [3  /  7,  2  /  7,  1  /  7,  1  /  7]\n",
    "    macro_F1 =  0.\n",
    "    for i in  range(len(weights)):\n",
    "        TP =  np.sum((label==i) & (prediction==i))\n",
    "        FP =  np.sum((label!= i) & (prediction == i))\n",
    "        FN =  np.sum((label == i) & (prediction!= i))\n",
    "        precision = TP /  (TP + FP)  if  (TP + FP)  >  0  else  0\n",
    "        recall = TP /  (TP + FN)  if  (TP + FN)  >  0  else  0\n",
    "        F1 =  2  * precision * recall /  (precision + recall)  if  (precision + recall)  >  0  else  0\n",
    "        macro_F1 += weights[i]  * F1\n",
    "        \n",
    "        print('Task %d:\\n Prcesion %.2f, Recall %.2f, F1 %.2f' % (i+1, precision, recall, F1))\n",
    "        \n",
    "    return macro_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3702b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "label=np.array(label_list)\n",
    "val_mask = [random.random() < 0.3 for _ in range(len(feature))]\n",
    "train_mask = [not xx for xx in val_mask]\n",
    "val_feature = feature[val_mask]\n",
    "val_label = label[val_mask]\n",
    "train_feature = feature[train_mask]\n",
    "train_label = label[train_mask]\n",
    "train_data=xgb.DMatrix(train_feature,label=train_label)\n",
    "train_feature=xgb.DMatrix(train_feature)\n",
    "val_feature=xgb.DMatrix(val_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "20ac33eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:14:51] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:14:51] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb_model=xgb.train(xgb_params,train_data,num_boost_round=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "bd630a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred=xgb_model.predict(train_feature)\n",
    "val_pred=xgb_model.predict(val_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "6c759e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1:\n",
      " Prcesion 0.92, Recall 0.57, F1 0.71\n",
      "Task 2:\n",
      " Prcesion 0.83, Recall 0.94, F1 0.88\n",
      "Task 3:\n",
      " Prcesion 0.98, Recall 0.99, F1 0.99\n",
      "Task 4:\n",
      " Prcesion 0.94, Recall 0.98, F1 0.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8323596122219039"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(train_label,train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b9327053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1:\n",
      " Prcesion 0.52, Recall 0.28, F1 0.37\n",
      "Task 2:\n",
      " Prcesion 0.72, Recall 0.81, F1 0.76\n",
      "Task 3:\n",
      " Prcesion 0.95, Recall 0.97, F1 0.96\n",
      "Task 4:\n",
      " Prcesion 0.89, Recall 0.93, F1 0.91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6416974838988792"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(val_label,val_pred)\n",
    "# 采用所有词的词频向量比采用词频+word2vec效果更佳"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317827d5",
   "metadata": {},
   "source": [
    "## 对测试集进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909bb0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Log_diagosis_python",
   "language": "python",
   "name": "log_diagosis_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
