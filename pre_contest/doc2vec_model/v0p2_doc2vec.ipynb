{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于V0P1进行调整，主要调整log和label对应关系，使用XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 引入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T03:45:34.007699Z",
     "start_time": "2022-03-04T03:45:19.522853Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T04:08:40.931146Z",
     "start_time": "2022-03-04T03:51:27.359416Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jinca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\workfile\\python\\Log-diagnosis\n"
     ]
    }
   ],
   "source": [
    "# 更改工作目录为当前项目根目录\n",
    "import sys\n",
    "import os\n",
    "os.chdir(os.path.dirname(os.path.dirname(sys.path[0])))\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 读取sel日志数据\n",
    "sel_log = pd.read_csv('./pre_contest/dataset/preliminary_sel_log_dataset.csv')\n",
    "\n",
    "# 读取训练标签数据：有重复数据！\n",
    "train_label1=pd.read_csv('./pre_contest/dataset/preliminary_train_label_dataset.csv')\n",
    "train_label2=pd.read_csv('./pre_contest/dataset/preliminary_train_label_dataset_s.csv')\n",
    "train_label=pd.concat([train_label1,train_label2],axis=0).drop_duplicates()\n",
    "\n",
    "# 读取日志语料数据\n",
    "additional_sel_log=pd.read_csv('./pre_contest/dataset/additional_sel_log_dataset.csv')\n",
    "\n",
    "# 所有去重的日志语料\n",
    "all_log_list=list(set(list(additional_sel_log['msg'].drop_duplicates())+list(sel_log['msg'].drop_duplicates())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关联sel_log和train_label：两张表的sn均可以匹配到，但是多对多\n",
    "# 一般是当天的sel_log全打出之后，当天晚些时间会报错，可以先只考虑一天只有一个报错信息的数据，一天多个报错的数据不纳入训练集和验证集\n",
    "# 总标签数据16604   sn和fault_day不同的15521   sn在fault_day唯一的标签数据14516   大概2000条标签数据不会用到\n",
    "# 按照sn+day分组\n",
    "train_label['day']=train_label['fault_time'].apply(lambda x:x[0:10])\n",
    "temp=train_label.groupby(['sn','day']).size()\n",
    "use_temp=temp[temp.values==1]\n",
    "sn_list=[use_temp.index[i][0] for i in range(len(use_temp))]\n",
    "day_list=[use_temp.index[i][1] for i in range(len(use_temp))]\n",
    "use_temp_df=pd.DataFrame({'sn':sn_list,'day':day_list})\n",
    "use_train_label=pd.merge(train_label,use_temp_df,how='inner',on=['sn','day'])\n",
    "sel_log['day']=sel_log['time'].apply(lambda x:x[0:10])\n",
    "use_log_label_df=pd.merge(sel_log,use_train_label,how='inner',on=['sn','day'])\n",
    "columns_order=['sn','day','time','msg','server_model','fault_time','label']\n",
    "use_log_label_df=use_log_label_df[columns_order]\n",
    "# 按照sn+day分组后按time排序\n",
    "# use_log_label_group_df=use_log_label_df.groupby(['sn','day']).apply(lambda x:x.sort_values('time',ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_str_list=[]\n",
    "label_list=[]\n",
    "for log_df in use_log_label_df.groupby(['sn','day']):\n",
    "    log_str=''\n",
    "    for info in log_df[1]['msg'].drop_duplicates():\n",
    "        log_str=log_str+info.lower()+'.'\n",
    "    log_label=log_df[1].iloc[0]['label']\n",
    "    log_str_list.append(log_str)\n",
    "    label_list.append(log_label)\n",
    "word_label_df=pd.DataFrame({'log':log_str_list,'label':label_list})\n",
    "word_label_df.to_csv('./pre_contest/data_analysis/word_label_df.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    1460\n",
       "1    3016\n",
       "2    7731\n",
       "3    2214\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_label_df.groupby('label').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 训练embbeding模型（Doc2Vec）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练Doc2Vec模型\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(all_log_list)]\n",
    "model = Doc2Vec(tagged_data, dm=1,vector_size = 200, window = 2, min_count = 1, epochs = 100,workers=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 构建树模型的训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'infer_vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m vector_list\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m word_label_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m----> 3\u001b[0m     vector_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_vector\u001b[49m(word_tokenize(log)))\n\u001b[0;32m      4\u001b[0m feature\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(vector_list)\n\u001b[0;32m      5\u001b[0m label\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(label_list)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'infer_vector'"
     ]
    }
   ],
   "source": [
    "vector_list=[]\n",
    "for log in word_label_df['log']:\n",
    "    vector_list.append(model.infer_vector(word_tokenize(log)))\n",
    "feature=np.array(vector_list)\n",
    "label=np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('./pre_contest/doc2vec_model/modelv0p2.model','wb')\n",
    "# pickle.dump(model, file)\n",
    "# file = open('./pre_contest/doc2vec_model/modelv0p2.feature','wb')\n",
    "# pickle.dump(feature, file)\n",
    "# file = open('./pre_contest/doc2vec_model/modelv0p2.label','wb')\n",
    "# pickle.dump(label_list, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 训练XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指标评估\n",
    "def macro_f1(label,prediction)  -> float:\n",
    "\n",
    "    \"\"\"\n",
    "    计算得分\n",
    "    :param target_df: [sn,fault_time,label]\n",
    "    :param submit_df: [sn,fault_time,label]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    weights =  [3  /  7,  2  /  7,  1  /  7,  1  /  7]\n",
    "    macro_F1 =  0.\n",
    "    for i in  range(len(weights)):\n",
    "        TP =  np.sum((label==i) & (prediction==i))\n",
    "        FP =  np.sum((label!= i) & (prediction == i))\n",
    "        FN =  np.sum((label == i) & (prediction!= i))\n",
    "        precision = TP /  (TP + FP)  if  (TP + FP)  >  0  else  0\n",
    "        recall = TP /  (TP + FN)  if  (TP + FN)  >  0  else  0\n",
    "        F1 =  2  * precision * recall /  (precision + recall)  if  (precision + recall)  >  0  else  0\n",
    "        macro_F1 += weights[i]  * F1\n",
    "        \n",
    "        print('Task %d:\\n Prcesion %.2f, Recall %.2f, F1 %.2f' % (i+1, precision, recall, F1))\n",
    "        \n",
    "    return macro_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T06:18:07.071782Z",
     "start_time": "2022-03-04T06:18:07.053125Z"
    }
   },
   "outputs": [],
   "source": [
    "# validation dataset\n",
    "random.seed(0)\n",
    "val_mask = [random.random() < 0.2 for _ in range(len(feature))]\n",
    "train_mask = [not xx for xx in val_mask]\n",
    "val_feature = feature[val_mask]\n",
    "val_label = label[val_mask]\n",
    "train_feature = feature[train_mask]\n",
    "train_label = label[train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=xgb.DMatrix(train_feature,label=train_label)\n",
    "train_feature=xgb.DMatrix(train_feature)\n",
    "val_feature=xgb.DMatrix(val_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb模型参数\n",
    "params = {\n",
    "    'booster':'gbtree',\n",
    "    'objective':'multi:softmax',   # 多分类问题\n",
    "    'num_class':4,  # 类别数，与multi softmax并用\n",
    "    'gamma':0.1,    # 用于控制是否后剪枝的参数，越大越保守，一般0.1 0.2的样子\n",
    "    'max_depth':6,  # 构建树的深度，越大越容易过拟合\n",
    "    'lambda':2,  # 控制模型复杂度的权重值的L2 正则化项参数，参数越大，模型越不容易过拟合\n",
    "    'subsample':1, # 随机采样训练样本\n",
    "    'colsample_bytree':1,# 这个参数默认为1，是每个叶子里面h的和至少是多少\n",
    "    # 对于正负样本不均衡时的0-1分类而言，假设h在0.01附近，min_child_weight为1\n",
    "    #意味着叶子节点中最少需要包含100个样本。这个参数非常影响结果，\n",
    "    # 控制叶子节点中二阶导的和的最小值，该参数值越小，越容易过拟合\n",
    "    'silent':0,  # 设置成1 则没有运行信息输入，最好是设置成0\n",
    "    'eta':0.3,  # 如同学习率\n",
    "    'seed':1000,\n",
    "    'nthread':12,  #CPU线程数\n",
    "    #'eval_metric':'auc'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:31:08] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[01:31:08] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb_model=xgb.train(params,train_data,num_boost_round=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred=xgb_model.predict(train_feature)\n",
    "val_pred=xgb_model.predict(val_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T06:19:40.057458Z",
     "start_time": "2022-03-04T06:19:39.947808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1:\n",
      " Prcesion 1.00, Recall 1.00, F1 1.00\n",
      "Task 2:\n",
      " Prcesion 1.00, Recall 1.00, F1 1.00\n",
      "Task 3:\n",
      " Prcesion 1.00, Recall 1.00, F1 1.00\n",
      "Task 4:\n",
      " Prcesion 1.00, Recall 1.00, F1 1.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(train_label,train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T06:20:12.837303Z",
     "start_time": "2022-03-04T06:20:12.790481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1:\n",
      " Prcesion 0.35, Recall 0.11, F1 0.16\n",
      "Task 2:\n",
      " Prcesion 0.49, Recall 0.47, F1 0.48\n",
      "Task 3:\n",
      " Prcesion 0.69, Recall 0.86, F1 0.77\n",
      "Task 4:\n",
      " Prcesion 0.63, Recall 0.45, F1 0.53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3918364597278289"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(val_label,val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 构建测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T05:58:23.915758Z",
     "start_time": "2022-03-04T05:53:28.800889Z"
    }
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./pre_contest/dataset/preliminary_submit_dataset_a.csv')\n",
    "submit.sort_values(by=['sn', 'fault_time'], inplace=True)\n",
    "submit.reset_index(drop=True, inplace=True)\n",
    "test_data = []\n",
    "for i, row in submit.iterrows():\n",
    "    test_data.append(model.infer_vector(word_tokenize('. '.join(sel_data[(sel_data['sn']==row['sn'])&(sel_data['time']<=row['fault_time'])].tail(10)['msg']).lower())))\n",
    "test_feature = np.array(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 预测并保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T05:58:47.554860Z",
     "start_time": "2022-03-04T05:58:47.273661Z"
    }
   },
   "outputs": [],
   "source": [
    "test_label = rf.predict(test_feature)\n",
    "submit['label'] = test_label\n",
    "submit.to_csv('./pre_contest/output/preliminary_pred_df.v1.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Log_diagosis_python",
   "language": "python",
   "name": "log_diagosis_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
