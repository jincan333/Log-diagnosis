{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于V0P1进行调整，主要调整log和label对应关系，使用XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 引入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T03:45:34.007699Z",
     "start_time": "2022-03-04T03:45:19.522853Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinca\\anaconda3\\envs\\Log_diagnosis_python\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\jinca\\anaconda3\\envs\\Log_diagnosis_python\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T04:08:40.931146Z",
     "start_time": "2022-03-04T03:51:27.359416Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jinca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\workfile\\python\\Log-diagnosis\n"
     ]
    }
   ],
   "source": [
    "# 更改工作目录为当前项目根目录\n",
    "import sys\n",
    "import os\n",
    "os.chdir(os.path.dirname(os.path.dirname(sys.path[0])))\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 读取sel日志数据\n",
    "sel_log = pd.read_csv('./pre_contest/dataset/preliminary_sel_log_dataset.csv')\n",
    "\n",
    "# 读取训练标签数据：有重复数据！\n",
    "train_label1=pd.read_csv('./pre_contest/dataset/preliminary_train_label_dataset.csv')\n",
    "train_label2=pd.read_csv('./pre_contest/dataset/preliminary_train_label_dataset_s.csv')\n",
    "train_label=pd.concat([train_label1,train_label2],axis=0).drop_duplicates()\n",
    "\n",
    "# 读取日志语料数据\n",
    "additional_sel_log=pd.read_csv('./pre_contest/dataset/additional_sel_log_dataset.csv')\n",
    "\n",
    "# 所有去重的日志语料\n",
    "all_log_list=list(set(list(additional_sel_log['msg'].drop_duplicates())+list(sel_log['msg'].drop_duplicates())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关联sel_log和train_label：两张表的sn均可以匹配到，但是多对多\n",
    "# 一般是当天的sel_log全打出之后，当天晚些时间会报错，可以先只考虑一天只有一个报错信息的数据，一天多个报错的数据不纳入训练集和验证集\n",
    "# 总标签数据16604   sn和fault_day不同的15521   sn在fault_day唯一的标签数据14516   大概2000条标签数据不会用到\n",
    "# 按照sn+day分组\n",
    "train_label['day']=train_label['fault_time'].apply(lambda x:x[0:10])\n",
    "temp=train_label.groupby(['sn','day']).size()\n",
    "use_temp=temp[temp.values==1]\n",
    "sn_list=[use_temp.index[i][0] for i in range(len(use_temp))]\n",
    "day_list=[use_temp.index[i][1] for i in range(len(use_temp))]\n",
    "use_temp_df=pd.DataFrame({'sn':sn_list,'day':day_list})\n",
    "use_train_label=pd.merge(train_label,use_temp_df,how='inner',on=['sn','day'])\n",
    "sel_log['day']=sel_log['time'].apply(lambda x:x[0:10])\n",
    "use_log_label_df=pd.merge(sel_log,use_train_label,how='inner',on=['sn','day'])\n",
    "columns_order=['sn','day','time','msg','server_model','fault_time','label']\n",
    "use_log_label_df=use_log_label_df[columns_order]\n",
    "# 按照sn+day分组后按time排序\n",
    "# use_log_label_group_df=use_log_label_df.groupby(['sn','day']).apply(lambda x:x.sort_values('time',ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_str_list=[]\n",
    "label_list=[]\n",
    "for log_df in use_log_label_df.groupby(['sn','day']):\n",
    "    for info in log_df[1]['msg'].drop_duplicates():\n",
    "        if len(info.lower().split('|'))==3:\n",
    "            log_str_list.append(info.lower().split('|'))\n",
    "            label_list.append(log_df[1].iloc[0]['label'])\n",
    "word2vec_label_df=pd.DataFrame({'log':log_str_list,'label':label_list})\n",
    "word2vec_label_df.to_csv('./pre_contest/data_analysis/word2vec_label_df.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    10625\n",
       "1    19845\n",
       "2    28336\n",
       "3    13584\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_label_df.groupby('label').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 训练embbeding模型（Word2Vec）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练Word2Vec模型\n",
    "model = Word2Vec(log_str_list,vector_size=100, alpha=0.03, window=10, min_count=1,max_vocab_size=None, sample=1e-3, seed=0, workers=12, min_alpha=0.0001,sg=1, hs=0, negative=5, cbow_mean=1, hashfxn=hash, epochs=10, null_word=0,trim_rule=None, sorted_vocab=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 构建树模型的训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_list=[]\n",
    "for log in word2vec_label_df['log']:\n",
    "    vector_list.append(model.wv[log].reshape(1,-1)[0])\n",
    "feature=np.array(vector_list)\n",
    "label=np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('./pre_contest/doc2vec_model/modelv0p2.model','wb')\n",
    "# pickle.dump(model, file)\n",
    "# file = open('./pre_contest/doc2vec_model/modelv0p2.feature','wb')\n",
    "# pickle.dump(feature, file)\n",
    "# file = open('./pre_contest/doc2vec_model/modelv0p2.label','wb')\n",
    "# pickle.dump(label_list, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 训练XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指标评估\n",
    "def macro_f1(label,prediction)  -> float:\n",
    "\n",
    "    \"\"\"\n",
    "    计算得分\n",
    "    :param target_df: [sn,fault_time,label]\n",
    "    :param submit_df: [sn,fault_time,label]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    weights =  [3  /  7,  2  /  7,  1  /  7,  1  /  7]\n",
    "    macro_F1 =  0.\n",
    "    for i in  range(len(weights)):\n",
    "        TP =  np.sum((label==i) & (prediction==i))\n",
    "        FP =  np.sum((label!= i) & (prediction == i))\n",
    "        FN =  np.sum((label == i) & (prediction!= i))\n",
    "        precision = TP /  (TP + FP)  if  (TP + FP)  >  0  else  0\n",
    "        recall = TP /  (TP + FN)  if  (TP + FN)  >  0  else  0\n",
    "        F1 =  2  * precision * recall /  (precision + recall)  if  (precision + recall)  >  0  else  0\n",
    "        macro_F1 += weights[i]  * F1\n",
    "        \n",
    "        print('Task %d:\\n Prcesion %.2f, Recall %.2f, F1 %.2f' % (i+1, precision, recall, F1))\n",
    "        \n",
    "    return macro_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T06:18:07.071782Z",
     "start_time": "2022-03-04T06:18:07.053125Z"
    }
   },
   "outputs": [],
   "source": [
    "# validation dataset\n",
    "random.seed(0)\n",
    "val_mask = [random.random() < 0.2 for _ in range(len(feature))]\n",
    "train_mask = [not xx for xx in val_mask]\n",
    "val_feature = feature[val_mask]\n",
    "val_label = label[val_mask]\n",
    "train_feature = feature[train_mask]\n",
    "train_label = label[train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=xgb.DMatrix(train_feature,label=train_label)\n",
    "train_feature=xgb.DMatrix(train_feature)\n",
    "val_feature=xgb.DMatrix(val_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb模型参数\n",
    "params = {\n",
    "    'booster':'gbtree',\n",
    "    'objective':'multi:softmax',   # 多分类问题\n",
    "    'num_class':4,  # 类别数，与multi softmax并用\n",
    "    'gamma':0.1,    # 用于控制是否后剪枝的参数，越大越保守，一般0.1 0.2的样子\n",
    "    'max_depth':6,  # 构建树的深度，越大越容易过拟合\n",
    "    'lambda':2,  # 控制模型复杂度的权重值的L2 正则化项参数，参数越大，模型越不容易过拟合\n",
    "    'subsample':1, # 随机采样训练样本\n",
    "    'colsample_bytree':1,# 这个参数默认为1，是每个叶子里面h的和至少是多少\n",
    "    # 对于正负样本不均衡时的0-1分类而言，假设h在0.01附近，min_child_weight为1\n",
    "    #意味着叶子节点中最少需要包含100个样本。这个参数非常影响结果，\n",
    "    # 控制叶子节点中二阶导的和的最小值，该参数值越小，越容易过拟合\n",
    "    'silent':0,  # 设置成1 则没有运行信息输入，最好是设置成0\n",
    "    'eta':0.3,  # 如同学习率\n",
    "    'seed':1000,\n",
    "    'nthread':12,  #CPU线程数\n",
    "    #'eval_metric':'auc'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:32:06] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[21:32:06] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb_model=xgb.train(params,train_data,num_boost_round=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred=xgb_model.predict(train_feature)\n",
    "val_pred=xgb_model.predict(val_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T06:19:40.057458Z",
     "start_time": "2022-03-04T06:19:39.947808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1:\n",
      " Prcesion 0.68, Recall 0.18, F1 0.28\n",
      "Task 2:\n",
      " Prcesion 0.53, Recall 0.70, F1 0.60\n",
      "Task 3:\n",
      " Prcesion 0.66, Recall 0.77, F1 0.71\n",
      "Task 4:\n",
      " Prcesion 0.70, Recall 0.52, F1 0.60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4797517115320911"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(train_label,train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T06:20:12.837303Z",
     "start_time": "2022-03-04T06:20:12.790481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1:\n",
      " Prcesion 0.57, Recall 0.15, F1 0.24\n",
      "Task 2:\n",
      " Prcesion 0.50, Recall 0.69, F1 0.58\n",
      "Task 3:\n",
      " Prcesion 0.66, Recall 0.75, F1 0.71\n",
      "Task 4:\n",
      " Prcesion 0.67, Recall 0.51, F1 0.58\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4510144281370416"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(val_label,val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 构建测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T05:58:23.915758Z",
     "start_time": "2022-03-04T05:53:28.800889Z"
    }
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./pre_contest/dataset/preliminary_submit_dataset_a.csv')\n",
    "submit.sort_values(by=['sn', 'fault_time'], inplace=True)\n",
    "submit.reset_index(drop=True, inplace=True)\n",
    "test_data = []\n",
    "for i, row in submit.iterrows():\n",
    "    test_data.append(model.infer_vector(word_tokenize('. '.join(sel_data[(sel_data['sn']==row['sn'])&(sel_data['time']<=row['fault_time'])].tail(10)['msg']).lower())))\n",
    "test_feature = np.array(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 预测并保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T05:58:47.554860Z",
     "start_time": "2022-03-04T05:58:47.273661Z"
    }
   },
   "outputs": [],
   "source": [
    "test_label = rf.predict(test_feature)\n",
    "submit['label'] = test_label\n",
    "submit.to_csv('./pre_contest/output/preliminary_pred_df.v1.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Log_diagosis_python",
   "language": "python",
   "name": "log_diagosis_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
