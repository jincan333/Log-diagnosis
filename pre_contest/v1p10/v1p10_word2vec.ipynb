{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c0ace7",
   "metadata": {},
   "source": [
    "# 背景：优化word2vec模型\n",
    "\n",
    "1、采用最终的日志和标签的匹配方式\n",
    "\n",
    "2、计算标签的embedding向量时，将权重由均值调整为加入时间间隔因子计算权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d7456b",
   "metadata": {},
   "source": [
    "## 导包、设置根目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de2893e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jincan02/Projects/Log-diagnosis\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from datetime import datetime \n",
    "\n",
    "# 更改工作目录为当前项目根目录\n",
    "import sys\n",
    "import os\n",
    "os.chdir(os.path.dirname(os.path.dirname(sys.path[0])))\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c9fee96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理后数据的主键为 sn + fault_time + label !!!\n",
    "\n",
    "# sn分组后，按照最近邻+时间间隔划分日志数据\n",
    "def divideLogByNearestTime(log_label_df: pd.DataFrame):\n",
    "    log_correspond_label_df = pd.DataFrame(columns=['sn', 'fault_time', 'msg', 'time', 'server_model', 'label'])\n",
    "    no_label_log_list = []\n",
    "    # 不设置截断时间，效果最好\n",
    "    cutoff = 10 * 3600\n",
    "\n",
    "    for sn, log in log_label_df.groupby('sn'):\n",
    "        if len(log[log['label'] != '']) == 0:\n",
    "            no_label_log_list.append(log)\n",
    "        elif len(log[log['label'] != '']) == 1:\n",
    "            # 没有日志的标签，直接将标签的可用信息作为日志\n",
    "            if len(log) == 1:\n",
    "                msg_df = log\n",
    "                msg_df['fault_time'] = log['time'].iloc[0]\n",
    "                log_correspond_label_df = pd.concat([log_correspond_label_df, msg_df])\n",
    "            else:\n",
    "                msg_df = log[log['label'] == '']\n",
    "                msg_df['label'] = log[log['label'] != '']['label'].iloc[0]\n",
    "                msg_df['fault_time'] = log[log['label'] != '']['time'].iloc[0]\n",
    "                log_correspond_label_df = pd.concat([log_correspond_label_df, msg_df])\n",
    "        else:\n",
    "            label_df = log[log['label'] != '']\n",
    "            msg_df = log[log['label'] == '']\n",
    "            for msg_item in msg_df.iterrows():\n",
    "                previous_delta_time = 1000 * 24 * 3600\n",
    "                for label_item in label_df.iterrows():\n",
    "                    now_delta_time = abs(datetime.strptime(label_item[1]['time'],'%Y-%m-%d %H:%M:%S'\n",
    "                        ) - datetime.strptime(msg_item[1]['time'],'%Y-%m-%d %H:%M:%S'))\n",
    "                    if now_delta_time.days * 24 * 3600 + now_delta_time.seconds < previous_delta_time:\n",
    "                        previous_delta_time = now_delta_time.days * 24 * 3600 + now_delta_time.seconds\n",
    "                        if previous_delta_time < cutoff:\n",
    "                            msg_item[1]['fault_time'] = label_item[1]['time']\n",
    "                            msg_item[1]['label'] = label_item[1]['label']\n",
    "            log_correspond_label_df = pd.concat([log_correspond_label_df, msg_df])\n",
    "            # 没有日志的标签，直接将标签的可用信息作为日志\n",
    "            for label_item in label_df.iterrows():\n",
    "                if len(msg_df[(msg_df['fault_time'] == label_item[1]['time']) & (\n",
    "                    msg_df['label'] == label_item[1]['label'])]) == 0:\n",
    "                    label_item[1]['fault_time'] = label_item[1]['time']\n",
    "            log_correspond_label_df = pd.concat([log_correspond_label_df, label_df])\n",
    "    if len(log_correspond_label_df[log_correspond_label_df['label'] == '']) > 0:\n",
    "        no_label_log_list.append(log_correspond_label_df[log_correspond_label_df['label'] == ''])\n",
    "    log_correspond_label_df = log_correspond_label_df[log_correspond_label_df['fault_time'] != '']\n",
    "    return log_correspond_label_df, no_label_log_list\n",
    "\n",
    "\n",
    "\n",
    "# sn分组后，本次报错和上次报错之间的日志匹配到本次报错\n",
    "def divideLogByFaultTime(log_label_df: pd.DataFrame):\n",
    "    log_correspond_label_df = pd.DataFrame(columns=['sn', 'fault_time', 'msg', 'time', 'server_model', 'label'])\n",
    "    no_label_log_list = []\n",
    "    log_label_df =  log_label_df.reset_index(drop = True)\n",
    "    \n",
    "    for sn, log in log_label_df.groupby('sn'):\n",
    "        if len(log[log['label'] != '']) == 0:\n",
    "            no_label_log_list.append(log)\n",
    "        elif len(log[log['label'] != '']) == 1:\n",
    "            msg_df = log[log['label'] == '']\n",
    "            msg_df['label'] = log[log['label'] != '']['label'].iloc[0]\n",
    "            msg_df['fault_time'] = log[log['label'] != '']['time'].iloc[0]\n",
    "            log_correspond_label_df = pd.concat([log_correspond_label_df, msg_df])\n",
    "        else:\n",
    "            # 使用index的顺序取数时，要注意index必须按所需的顺序排列\n",
    "            cutoff_index = [-1] + log.loc[log['label'] != ''].index.tolist() + [log.index.tolist()[-1]+1]\n",
    "            for kth in range(len(cutoff_index)-1):\n",
    "                temp_log = log.loc[(log.index <= cutoff_index[kth+1]) & (log.index > cutoff_index[kth])]\n",
    "                if len(temp_log) > 0:\n",
    "                    if len(temp_log[temp_log['label'] != '']) == 0:\n",
    "                        no_label_log_list.append(temp_log)\n",
    "                    # 只有标签，没有日志的数据，把标签的部分数据直接作为日志\n",
    "                    elif len(temp_log) == 1:\n",
    "                        msg_df = temp_log\n",
    "                        msg_df['fault_time'] = temp_log[temp_log['label'] != '']['time'].iloc[0]\n",
    "                        log_correspond_label_df = pd.concat([log_correspond_label_df, msg_df])\n",
    "                    else:\n",
    "                        msg_df = temp_log[temp_log['label'] == '']\n",
    "                        msg_df['label'] = temp_log[temp_log['label'] != '']['label'].iloc[0]\n",
    "                        msg_df['fault_time'] = temp_log[temp_log['label'] != '']['time'].iloc[0]\n",
    "                        log_correspond_label_df = pd.concat([log_correspond_label_df, msg_df])\n",
    "    return log_correspond_label_df, no_label_log_list\n",
    "\n",
    "\n",
    "\n",
    "# 计算统计特征\n",
    "def calculateStatisticFeature(log_correspond_label_df: pd.DataFrame):\n",
    "    use_log_label_df = log_correspond_label_df\n",
    "\n",
    "    use_log_label_df['msg_hour'] = use_log_label_df['time'].apply(lambda x : datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").hour)\n",
    "    use_log_label_df['msg_minute'] = use_log_label_df['time'].apply(lambda x : datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").minute)\n",
    "    use_log_label_df['fault_hour'] = use_log_label_df['fault_time'].apply(lambda x : datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").hour)\n",
    "    use_log_label_df['fault_minute'] = use_log_label_df['fault_time'].apply(lambda x : datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").minute)\n",
    "\n",
    "    # 0414新增\n",
    "    # 不去重msg_log\n",
    "    all_msg_log_list = []\n",
    "    \n",
    "    # 0408新增\n",
    "    # 最近一次日志时间距报错时间间隔，单位秒\n",
    "    nearest_msg_fault_time_delta_list = []\n",
    "    # 日志不去重时长度1,2,3,4日志数量统计\n",
    "    all_msg_1_cnt_list=[]\n",
    "    all_msg_2_cnt_list=[]\n",
    "    all_msg_3_cnt_list=[]\n",
    "    all_msg_4_cnt_list=[]\n",
    "    \n",
    "    fault_minute_list = []\n",
    "    msg_1_cnt_list=[]\n",
    "    msg_2_cnt_list=[]\n",
    "    msg_3_cnt_list=[]\n",
    "    msg_4_cnt_list=[]\n",
    "    msg_hour_max_list=[]\n",
    "    msg_hour_min_list=[]\n",
    "    msg_hour_avg_list=[]\n",
    "    msg_hour_median_list=[]\n",
    "    msg_hour_mode_list=[]\n",
    "    msg_minute_max_list=[]\n",
    "    msg_minute_min_list=[]\n",
    "    msg_minute_avg_list=[]\n",
    "    msg_minute_median_list=[]\n",
    "    msg_minute_mode_list=[]\n",
    "\n",
    "    sn_list=[]\n",
    "    day_list=[]\n",
    "    server_model_list=[]\n",
    "    msg_log_list=[]\n",
    "    msg_cnt_list=[]\n",
    "    fault_hour_list=[]\n",
    "    label_list=[]\n",
    "    fault_time_list=[]\n",
    "    for msg_log_df in use_log_label_df.groupby(['sn','fault_time','label']):\n",
    "        all_msg_log_str = ''\n",
    "        msg_log_str = ''\n",
    "        all_msg_1_cnt = 0\n",
    "        all_msg_2_cnt = 0\n",
    "        all_msg_3_cnt = 0\n",
    "        all_msg_4_cnt = 0\n",
    "        msg_1_cnt = 0\n",
    "        msg_2_cnt = 0\n",
    "        msg_3_cnt = 0\n",
    "        msg_4_cnt = 0\n",
    "        for info in msg_log_df[1]['msg']:\n",
    "            if info == info:\n",
    "                all_msg_log_str = all_msg_log_str + info.lower() + '.'\n",
    "                if len(info.split('|')) == 1:\n",
    "                    all_msg_1_cnt += 1\n",
    "                elif len(info.split('|')) == 2:\n",
    "                    all_msg_2_cnt += 1\n",
    "                elif len(info.split('|')) == 3:\n",
    "                    all_msg_3_cnt += 1\n",
    "                else:\n",
    "                    all_msg_4_cnt += 1\n",
    "        for info in msg_log_df[1]['msg'].drop_duplicates():\n",
    "            if info == info:\n",
    "                msg_log_str=msg_log_str+info.lower()+'.'\n",
    "                if len(info.split('|')) == 1:\n",
    "                    msg_1_cnt += 1\n",
    "                elif len(info.split('|')) == 2:\n",
    "                    msg_2_cnt += 1\n",
    "                elif len(info.split('|')) == 3:\n",
    "                    msg_3_cnt += 1\n",
    "                else:\n",
    "                    msg_4_cnt += 1\n",
    "        nearest_msg_fault_time_delta = abs(datetime.strptime(msg_log_df[1].iloc[-1]['time'],'%Y-%m-%d %H:%M:%S'\n",
    "                        ) - datetime.strptime(msg_log_df[0][1],'%Y-%m-%d %H:%M:%S'))\n",
    "        nearest_msg_fault_time_delta = nearest_msg_fault_time_delta.days * 24 * 3600 + nearest_msg_fault_time_delta.seconds\n",
    "        sm=int(msg_log_df[1].iloc[0]['server_model'][2:])\n",
    "\n",
    "        sn_list.append(msg_log_df[0][0])\n",
    "        fault_time_list.append(msg_log_df[0][1])\n",
    "        label_list.append(msg_log_df[0][2])\n",
    "\n",
    "        nearest_msg_fault_time_delta_list.append(nearest_msg_fault_time_delta)\n",
    "        server_model_list.append(sm)\n",
    "        all_msg_log_list.append(all_msg_log_str)\n",
    "        msg_log_list.append(msg_log_str)\n",
    "        msg_cnt_list.append(len(msg_log_df[1]))\n",
    "\n",
    "        fault_hour_list.append(msg_log_df[1].iloc[0]['fault_hour'])\n",
    "        fault_minute_list.append(msg_log_df[1].iloc[0]['fault_minute'])\n",
    "\n",
    "        all_msg_1_cnt_list.append(all_msg_1_cnt)\n",
    "        all_msg_2_cnt_list.append(all_msg_2_cnt)\n",
    "        all_msg_3_cnt_list.append(all_msg_3_cnt)\n",
    "        all_msg_4_cnt_list.append(all_msg_4_cnt)    \n",
    "\n",
    "        msg_1_cnt_list.append(msg_1_cnt)\n",
    "        msg_2_cnt_list.append(msg_2_cnt)\n",
    "        msg_3_cnt_list.append(msg_3_cnt)\n",
    "        msg_4_cnt_list.append(msg_4_cnt)\n",
    "\n",
    "        msg_hour_max_list.append(msg_log_df[1]['msg_hour'].max())\n",
    "        msg_hour_min_list.append(msg_log_df[1]['msg_hour'].min())\n",
    "        msg_hour_avg_list.append(msg_log_df[1]['msg_hour'].mean())\n",
    "        msg_hour_median_list.append(msg_log_df[1]['msg_hour'].median())\n",
    "        msg_hour_mode_list.append(msg_log_df[1]['msg_hour'].mode()[0])\n",
    "\n",
    "        msg_minute_max_list.append(msg_log_df[1]['msg_minute'].max())\n",
    "        msg_minute_min_list.append(msg_log_df[1]['msg_minute'].min())\n",
    "        msg_minute_avg_list.append(msg_log_df[1]['msg_minute'].mean())\n",
    "        msg_minute_median_list.append(msg_log_df[1]['msg_minute'].median())\n",
    "        msg_minute_mode_list.append(msg_log_df[1]['msg_minute'].mode()[0])\n",
    "\n",
    "    msg_log_label_df=pd.DataFrame(\n",
    "        {\n",
    "        'sn': sn_list,\n",
    "        'fault_time': fault_time_list,\n",
    "        'server_model': server_model_list,\n",
    "        'msg_cnt': msg_cnt_list,\n",
    "        'fault_hour': fault_hour_list,\n",
    "        'fault_minute': fault_minute_list,\n",
    "        'nearest_msg_fault_time_delta': nearest_msg_fault_time_delta_list,\n",
    "        'all_msg_1_cnt': all_msg_1_cnt_list,\n",
    "        'all_msg_2_cnt': all_msg_2_cnt_list,\n",
    "        'all_msg_3_cnt': all_msg_3_cnt_list,\n",
    "        'all_msg_4_cnt': all_msg_4_cnt_list,\n",
    "        'msg_1_cnt': msg_1_cnt_list,\n",
    "        'msg_2_cnt': msg_2_cnt_list,\n",
    "        'msg_3_cnt': msg_3_cnt_list,\n",
    "        'msg_4_cnt': msg_4_cnt_list,\n",
    "        'msg_hour_max': msg_hour_max_list,\n",
    "        'msg_hour_min': msg_hour_min_list,\n",
    "        'msg_hour_avg': msg_hour_avg_list,\n",
    "        'msg_hour_median': msg_hour_median_list,\n",
    "        'msg_hour_mode': msg_hour_mode_list,\n",
    "        'msg_minute_max': msg_minute_max_list,\n",
    "        'msg_minute_min': msg_minute_min_list,\n",
    "        'msg_minute_avg': msg_minute_avg_list,\n",
    "        'msg_minute_median': msg_minute_median_list,\n",
    "        'msg_minute_mode': msg_minute_mode_list,\n",
    "        'msg_log': msg_log_list,\n",
    "        'all_msg_log': all_msg_log_list,\n",
    "        'label': label_list\n",
    "        }\n",
    "    )\n",
    "    return msg_log_label_df\n",
    "\n",
    "\n",
    "# 使用去除标点符号之后的词训练word2vec模型和embedding向量\n",
    "def TrainDropPuncWord2vecFeature(all_msg_log_list: list, FaultTime_log_correspond_label_df: pd.DataFrame) -> list:\n",
    "    # 对日志进行分词\n",
    "    raw_word_list = [word_tokenize(ith) for ith in all_msg_log_list]\n",
    "    word_list=[]\n",
    "    for i in range(len(raw_word_list)):\n",
    "        xth=[]\n",
    "        for word in raw_word_list[i]:\n",
    "            word_drop=re.sub(r'[^\\w]','',str(word)).lower()\n",
    "            if word_drop:\n",
    "                xth.append(word_drop)\n",
    "        word_list.append(xth)\n",
    "    # 使用去除空格和标签符号后的词，训练word2vec模型\n",
    "    word2vec_model = Word2Vec(word_list,vector_size=100, alpha=0.03, window=5, min_count=1,max_vocab_size=None, sample=1e-3, seed=0, workers=12, min_alpha=0.0001,sg=1, hs=0, negative=5, cbow_mean=1, hashfxn=hash, epochs=50, null_word=0,trim_rule=None, sorted_vocab=1)\n",
    "    \n",
    "    # 训练词向量\n",
    "    word2vec_vector_list=[]\n",
    "    sn_list = []\n",
    "    fault_time_list = []\n",
    "    label_list = []\n",
    "\n",
    "    # 使用去除空格和标点符号的分词方式\n",
    "    for key, msg_log in FaultTime_log_correspond_label_df.groupby(['sn','fault_time','label']):\n",
    "        sn_list.append(key[0])\n",
    "        fault_time_list.append(key[1])\n",
    "        label_list.append(key[2])\n",
    "        embedding_vector_weighted_sum=[]\n",
    "        last_msg_delta_time = datetime.strptime(msg_log.iloc[-1]['fault_time'],'%Y-%m-%d %H:%M:%S'\n",
    "                            ) - datetime.strptime(msg_log.iloc[-1]['time'],'%Y-%m-%d %H:%M:%S')\n",
    "        last_msg_delta_seconds = last_msg_delta_time.days * 24 * 60 * 60 + last_msg_delta_time.seconds\n",
    "        if last_msg_delta_seconds == 0:\n",
    "            last_msg_delta_seconds = 1\n",
    "        for i, item in msg_log.iterrows():\n",
    "            # 计算权重\n",
    "            delta_time = datetime.strptime(item['fault_time'],'%Y-%m-%d %H:%M:%S'\n",
    "                            ) - datetime.strptime(item['time'],'%Y-%m-%d %H:%M:%S')\n",
    "            delta_seconds = delta_time.days * 24 * 60 * 60 + delta_time.seconds\n",
    "            if delta_seconds == 0:\n",
    "                delta_seconds = 1\n",
    "            weight = last_msg_delta_seconds / delta_seconds\n",
    "            # 对本条日志进行分词，去除空格和标点符号\n",
    "            item_raw_word_list = word_tokenize(item['msg'])\n",
    "            for word in item_raw_word_list:\n",
    "                word_drop=re.sub(r'[^\\w]','',str(word)).lower()\n",
    "                if word_drop:\n",
    "                    vector=word2vec_model.wv[word_drop].reshape(1,-1)[0]\n",
    "                    if len(embedding_vector_weighted_sum)>0:\n",
    "                        embedding_vector_weighted_sum=list(np.array(embedding_vector_weighted_sum)+weight * np.array(vector))\n",
    "                    else:\n",
    "                        embedding_vector_weighted_sum=list(np.array(vector))\n",
    "        if len(embedding_vector_weighted_sum) == 0:\n",
    "            embedding_vector_weighted_sum = np.array([0]*100)\n",
    "        word2vec_vector_list.append(embedding_vector_weighted_sum)\n",
    "    \n",
    "    return word2vec_vector_list\n",
    "\n",
    "\n",
    "\n",
    "# 使用原始分词后的词训练word2vec模型和embedding向量\n",
    "def TrainAllWord2vecFeature(all_msg_log_list: list, FaultTime_log_correspond_label_df: pd.DataFrame) -> list:\n",
    "    # 对日志进行分词\n",
    "    raw_word_list = [word_tokenize(ith) for ith in all_msg_log_list]\n",
    "    word2vec_model = Word2Vec(raw_word_list,vector_size=100, alpha=0.03, window=5, min_count=1,max_vocab_size=None, sample=1e-3, seed=0, workers=12, min_alpha=0.0001,sg=1, hs=0, negative=5, cbow_mean=1, hashfxn=hash, epochs=50, null_word=0,trim_rule=None, sorted_vocab=1)\n",
    "    \n",
    "    # 训练词向量\n",
    "    word2vec_vector_list=[]\n",
    "    sn_list = []\n",
    "    fault_time_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for key, msg_log in FaultTime_log_correspond_label_df.groupby(['sn','fault_time','label']):\n",
    "        sn_list.append(key[0])\n",
    "        fault_time_list.append(key[1])\n",
    "        label_list.append(key[2])\n",
    "        embedding_vector_weighted_sum=[]\n",
    "        last_msg_delta_time = datetime.strptime(msg_log.iloc[-1]['fault_time'],'%Y-%m-%d %H:%M:%S'\n",
    "                            ) - datetime.strptime(msg_log.iloc[-1]['time'],'%Y-%m-%d %H:%M:%S')\n",
    "        last_msg_delta_seconds = last_msg_delta_time.days * 24 * 60 * 60 + last_msg_delta_time.seconds\n",
    "        if last_msg_delta_seconds == 0:\n",
    "            last_msg_delta_seconds = 1\n",
    "        for i, item in msg_log.iterrows():\n",
    "            # 计算权重\n",
    "            delta_time = datetime.strptime(item['fault_time'],'%Y-%m-%d %H:%M:%S'\n",
    "                            ) - datetime.strptime(item['time'],'%Y-%m-%d %H:%M:%S')\n",
    "            delta_seconds = delta_time.days * 24 * 60 * 60 + delta_time.seconds\n",
    "            if delta_seconds == 0:\n",
    "                delta_seconds = 1\n",
    "            weight = last_msg_delta_seconds / delta_seconds\n",
    "            # 对本条日志进行分词\n",
    "            item_raw_word_list = word_tokenize(item['msg'])\n",
    "            for word in item_raw_word_list:\n",
    "                vector=word2vec_model.wv[word].reshape(1,-1)[0]\n",
    "                if len(embedding_vector_weighted_sum)>0:\n",
    "                    embedding_vector_weighted_sum=list(np.array(embedding_vector_weighted_sum)+weight * np.array(vector))\n",
    "                else:\n",
    "                    embedding_vector_weighted_sum=list(np.array(vector))\n",
    "        if len(embedding_vector_weighted_sum) == 0:\n",
    "            embedding_vector_weighted_sum = np.array([0]*100)\n",
    "        word2vec_vector_list.append(embedding_vector_weighted_sum)\n",
    "    \n",
    "    return word2vec_vector_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c455b9",
   "metadata": {},
   "source": [
    "## 采用按照fault_time划分日志的方式匹配标签和日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deed21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取sel日志数据\n",
    "sel_log_df = pd.read_csv('./pre_contest/dataset_B/preliminary_sel_log_dataset.csv').drop_duplicates()\n",
    "# 读取额外的日志数据\n",
    "additional_sel_log_df=pd.read_csv('./pre_contest/dataset_B/additional_sel_log_dataset.csv').drop_duplicates()\n",
    "# 读取训练标签数据：有重复数据！\n",
    "train_label1 = pd.read_csv('./pre_contest/dataset_B/preliminary_train_label_dataset.csv')\n",
    "train_label2 = pd.read_csv('./pre_contest/dataset_B/preliminary_train_label_dataset_s.csv')\n",
    "train_label_df = pd.concat([train_label1,train_label2],axis=0).drop_duplicates()\n",
    "\n",
    "\n",
    "# 合并日志和标签\n",
    "sel_log_df['label'] = ''\n",
    "train_label_df['time'] = train_label_df['fault_time']\n",
    "train_label_df['msg'] = ''\n",
    "train_label_df['server_model'] = train_label_df['sn'].map(dict(zip(sel_log_df['sn'],sel_log_df['server_model'])))\n",
    "train_label_df = train_label_df[['sn', 'time', 'msg', 'server_model', 'label']]\n",
    "\n",
    "log_label_df = pd.concat([sel_log_df,train_label_df], axis = 0).sort_values(by = 'time')\n",
    "log_label_df['fault_time'] = ''\n",
    "log_label_df = log_label_df[['sn', 'fault_time', 'msg', 'time', 'server_model', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de2e01fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bh/sv71wm991pb8sm02bw8rmrfc0000gp/T/ipykernel_81453/1189615245.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  msg_df['label'] = log[log['label'] != '']['label'].iloc[0]\n",
      "/var/folders/bh/sv71wm991pb8sm02bw8rmrfc0000gp/T/ipykernel_81453/1189615245.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  msg_df['fault_time'] = log[log['label'] != '']['time'].iloc[0]\n",
      "/var/folders/bh/sv71wm991pb8sm02bw8rmrfc0000gp/T/ipykernel_81453/1189615245.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  msg_df['label'] = temp_log[temp_log['label'] != '']['label'].iloc[0]\n",
      "/var/folders/bh/sv71wm991pb8sm02bw8rmrfc0000gp/T/ipykernel_81453/1189615245.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  msg_df['fault_time'] = temp_log[temp_log['label'] != '']['time'].iloc[0]\n",
      "/var/folders/bh/sv71wm991pb8sm02bw8rmrfc0000gp/T/ipykernel_81453/1189615245.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  msg_df['fault_time'] = temp_log[temp_log['label'] != '']['time'].iloc[0]\n"
     ]
    }
   ],
   "source": [
    "FaultTime_log_correspond_label_df, FaultTime_no_label_log_list = divideLogByFaultTime(log_label_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263e661",
   "metadata": {},
   "source": [
    "## 训练word2vec模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1136ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对所有日志进行去重\n",
    "all_msg_log_list = list(set(list(additional_sel_log_df['msg'].drop_duplicates()) + list(sel_log_df['msg'].drop_duplicates())))\n",
    "# 对日志进行分词\n",
    "raw_word_list = [word_tokenize(ith) for ith in all_msg_log_list]\n",
    "word_list=[]\n",
    "for i in range(len(raw_word_list)):\n",
    "    xth=[]\n",
    "    for word in raw_word_list[i]:\n",
    "        word_drop=re.sub(r'[^\\w]','',str(word)).lower()\n",
    "        if word_drop:\n",
    "            xth.append(word_drop)\n",
    "    word_list.append(xth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3660ccbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9421"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_msg_log_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34f4dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练Word2Vec模型\n",
    "# 使用未经处理的分词\n",
    "# word2vec_model = Word2Vec(raw_word_list,vector_size=100, alpha=0.03, window=5, min_count=1,max_vocab_size=None, sample=1e-3, seed=0, workers=12, min_alpha=0.0001,sg=1, hs=0, negative=5, cbow_mean=1, hashfxn=hash, epochs=50, null_word=0,trim_rule=None, sorted_vocab=1)\n",
    "# 使用去除空格和标签符号后的分词\n",
    "word2vec_model = Word2Vec(word_list,vector_size=100, alpha=0.03, window=5, min_count=1,max_vocab_size=None, sample=1e-3, seed=0, workers=12, min_alpha=0.0001,sg=1, hs=0, negative=5, cbow_mean=1, hashfxn=hash, epochs=50, null_word=0,trim_rule=None, sorted_vocab=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9c98a5",
   "metadata": {},
   "source": [
    "## 对日志进行embedding，加权求和得到特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f3323301",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_vector_list = TrainAllWord2vecFeature(all_msg_log_list, FaultTime_log_correspond_label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bbf2efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature=np.array(word2vec_vector_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cacd2f",
   "metadata": {},
   "source": [
    "## 训练xgb模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0b292500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb模型参数\n",
    "xgb_params = {\n",
    "    'booster':'gbtree',\n",
    "    'objective':'multi:softmax',   # 多分类问题\n",
    "    'num_class':4,  # 类别数，与multi softmax并用\n",
    "    'gamma':0.1,    # 用于控制是否后剪枝的参数，越大越保守，一般0.1 0.2的样子\n",
    "    'max_depth':6,  # 构建树的深度，越大越容易过拟合\n",
    "    'lambda':2,  # 控制模型复杂度的权重值的L2 正则化项参数，参数越大，模型越不容易过拟合\n",
    "    'subsample':1, # 随机采样训练样本\n",
    "    'colsample_bytree':1,# 这个参数默认为1，是每个叶子里面h的和至少是多少\n",
    "    # 对于正负样本不均衡时的0-1分类而言，假设h在0.01附近，min_child_weight为1\n",
    "    #意味着叶子节点中最少需要包含100个样本。这个参数非常影响结果，\n",
    "    # 控制叶子节点中二阶导的和的最小值，该参数值越小，越容易过拟合\n",
    "    'silent':0,  # 设置成1 则没有运行信息输入，最好是设置成0\n",
    "    'eta':0.3,  # 如同学习率\n",
    "    'seed':1000,\n",
    "    'nthread':16,  #CPU线程数\n",
    "    #'eval_metric':'auc'\n",
    "}\n",
    "\n",
    "# 指标评估\n",
    "def macro_f1(label,prediction)  -> float:\n",
    "\n",
    "    \"\"\"\n",
    "    计算得分\n",
    "    :param target_df: [sn,fault_time,label]\n",
    "    :param submit_df: [sn,fault_time,label]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    weights =  [3  /  7,  2  /  7,  1  /  7,  1  /  7]\n",
    "    macro_F1 =  0.\n",
    "    for i in  range(len(weights)):\n",
    "        TP =  np.sum((label==i) & (prediction==i))\n",
    "        FP =  np.sum((label!= i) & (prediction == i))\n",
    "        FN =  np.sum((label == i) & (prediction!= i))\n",
    "        precision = TP /  (TP + FP)  if  (TP + FP)  >  0  else  0\n",
    "        recall = TP /  (TP + FN)  if  (TP + FN)  >  0  else  0\n",
    "        F1 =  2  * precision * recall /  (precision + recall)  if  (precision + recall)  >  0  else  0\n",
    "        macro_F1 += weights[i]  * F1\n",
    "        \n",
    "        print('Task %d:\\n Prcesion %.2f, Recall %.2f, F1 %.2f' % (i+1, precision, recall, F1))\n",
    "        \n",
    "    return macro_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3702b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "label=np.array(label_list)\n",
    "val_mask = [random.random() < 0.3 for _ in range(len(feature))]\n",
    "train_mask = [not xx for xx in val_mask]\n",
    "val_feature = feature[val_mask]\n",
    "val_label = label[val_mask]\n",
    "train_feature = feature[train_mask]\n",
    "train_label = label[train_mask]\n",
    "train_data=xgb.DMatrix(train_feature,label=train_label)\n",
    "train_feature=xgb.DMatrix(train_feature)\n",
    "val_feature=xgb.DMatrix(val_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "20ac33eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:33:14] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[02:33:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb_model=xgb.train(xgb_params,train_data,num_boost_round=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bd630a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred=xgb_model.predict(train_feature)\n",
    "val_pred=xgb_model.predict(val_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6c759e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1:\n",
      " Prcesion 0.99, Recall 0.93, F1 0.96\n",
      "Task 2:\n",
      " Prcesion 0.97, Recall 0.98, F1 0.97\n",
      "Task 3:\n",
      " Prcesion 0.98, Recall 1.00, F1 0.99\n",
      "Task 4:\n",
      " Prcesion 0.99, Recall 0.96, F1 0.97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9716585704102343"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(train_label,train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b9327053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1:\n",
      " Prcesion 0.46, Recall 0.23, F1 0.31\n",
      "Task 2:\n",
      " Prcesion 0.65, Recall 0.71, F1 0.68\n",
      "Task 3:\n",
      " Prcesion 0.90, Recall 0.95, F1 0.92\n",
      "Task 4:\n",
      " Prcesion 0.84, Recall 0.81, F1 0.83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5756302442328277"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_f1(val_label,val_pred)\n",
    "# 使用去除标签符号的单词训练\n",
    "# 0.586243722832391\n",
    "\n",
    "# 使用分词后的原始单词训练\n",
    "# 0.5756302442328277"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Log_diagnosis",
   "language": "python",
   "name": "log_diagnosis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
